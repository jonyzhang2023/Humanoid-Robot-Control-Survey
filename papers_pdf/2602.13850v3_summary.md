# Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement

**Authors:** Minku Kim, Kuan-Chia Chen, Aayam Shrestha, Li Fuxin, Stefan Lee, Alan Fern
**Link:** http://arxiv.org/abs/2602.13850v3

## Abstract (from PDF parsing)
...

## Method / Approach (Snippets)
We compare approaches for adapting the pretrained MHC as new skills are added. Across methods, the learned high- level skill policies are fixed; only the whole-body controller (WBC) is updated (or not) to better cover the state/command distributions induced by the skills. The reason we chose this experiment is that, for humanoids, most of the balancing tasks fall within the realm of the WBC – the skill policies only output upper body arm commands, torso height, and pitch rotation, and they usually do a good job on those. However,  a WBC that is not trained with enough scenarios can easily lead the humanoid to lose balance, resulting in skill failure. Namely, we compare: Base (Frozen MHC). Base uses the pretrained MHC as a frozen shared WBC with no skill-specific adaptation. Finetune. Finetune adapts the WBC separately for each skill by fine-tuning from the pretrained MHC using the skill’s RL reward, rather than the MHC tracking objective. This yields skill-specific WBCs that are switched with the active skill. Residual. Residual freezes the pretrained MHC and learns a skill-specific residual policy optimized with the corresponding skill reward. This is similar to [20], except the target motions are generated by skill policies rather than mocap. These skill policies generate good trajectories on our tasks, especially without domain randomization. Residual uses an action scaling factor of 0.5, and reduced exploration noise for stability. One residual policy is learned for each ...

## Conclusion (Snippets)
...

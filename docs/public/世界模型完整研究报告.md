# 世界模型（World Model）完整研究报告

> **版本**：v1.0 · **更新日期**：2026 年 2 月 24 日
> **覆盖范围**：1943–2026 年 2 月 · **收录论文**：~210 篇
> **更新频率**：每月一次 · **下次计划更新**：2026 年 3 月
> **数据来源**：arXiv、顶会论文（NeurIPS / ICLR / ICML / CVPR / ICCV / ECCV / RSS / CoRL）、Nature、GitHub、企业技术博客

---


# 执行摘要

**世界模型（World Model）** 是人工智能系统中对外部环境进行内部建模的核心组件——它能够理解当前状态、预测未来变化、并模拟行动的后果。这一概念可追溯至认知科学家 Kenneth Craik（1943）提出的"心智模型"假说，而在 AI 领域则始于 Schmidhuber（1989）的 RNN 环境建模工作，并在 Ha & Schmidhuber（2018）的经典论文中被正式定义和系统化。

**截至 2026 年 2 月，世界模型研究已经历三次范式跃迁：**

| 阶段 | 时间 | 代表工作 | 核心特征 |
|------|------|---------|---------|
| **1.0 时代** | 1989–2022 | Schmidhuber (1989)、Ha & Schmidhuber (2018)、PlaNet、Dreamer 系列、[MuZero](https://arxiv.org/abs/1911.08265) | 任务特定的 RL 世界模型，在潜在空间中学习环境动力学 |
| **2.0 时代** | 2023–2024 | [Sora](https://openai.com/index/video-generation-models-as-world-simulators/)、[Genie](https://arxiv.org/abs/2402.15391)、[DreamerV3](https://arxiv.org/abs/2301.04104)、[GAIA-1](https://arxiv.org/abs/2309.17080) | 基础世界模型（Foundation World Model），在互联网规模的视频数据上训练，涌现出世界模拟能力 |
| **3.0 时代** | 2025– | [NVIDIA Cosmos](https://arxiv.org/abs/2501.03575)、[Genie 3](https://deepmind.google/blog/genie-3-a-new-frontier-for-world-models/)、[Sora 2](https://openai.com/blog/sora-2)、[VideoWorld](https://arxiv.org/abs/2501.09781)、[Aether](https://arxiv.org/abs/2503.18945)、[DreamZero](https://arxiv.org/abs/2602.15922) | 开源世界模型平台、实时交互式生成、3D/4D 几何感知、VLA+WM 融合、具身 AI 世界模型训练 |

**六大标志性事件定义了当前格局：**

1. **OpenAI Sora / Sora 2（2024.2 → 2025.9）**：证明扩展视频扩散 Transformer 可以涌现出 3D 一致性、物体持久性、基础物理等世界模拟能力；Sora 2 大幅提升物理精度并支持多镜头复杂指令和逼真音频。

2. **Google DeepMind Genie 1→2→3（2024.2 → 2024.12 → 2025.8）**：实现了从单张图片生成动作可控的交互式 3D 世界；Genie 3 成为**首个实时交互世界模型**（24fps/720p），标志着世界模型的新前沿。

3. **NVIDIA Cosmos 平台（2025.1–）**：发布了首个面向"物理 AI"（机器人、自动驾驶）的开源世界基础模型平台（13 个仓库，Apache-2.0），后续推出 Predict 2.5、Reason1、Transfer1、Drive-Dreams 等升级。

4. **具身 AI 世界模型爆发（2025–2026）**：[DreamZero](https://arxiv.org/abs/2602.15922)（14B 参数世界动作模型，7Hz 实时闭环控制）、[World-Gymnast](https://arxiv.org/abs/2602.02454)（在世界模型中 RL 微调 VLA，比 SFT 提升 18×）、[FLARE](https://arxiv.org/abs/2505.15659)（隐式世界模型，近零推理开销）等工作将"在世界模型中训练机器人策略"从理论推向实践，VLA + World Model 融合成为主流范式。

5. **视频基座模型全球竞争（2024–2025）**：智谱（[CogVideoX](https://arxiv.org/abs/2408.06072)，ICLR 2025）、腾讯（[HunyuanVideo](https://arxiv.org/abs/2412.03603)，130 亿参数）、阿里（[Wan2.1](https://arxiv.org/abs/2503.20314)，15.4k ⭐）、昆仑万维（[SkyReels-V2](https://arxiv.org/abs/2504.13074)，VBench SOTA 83.9%）、字节（Seedance）、快手（可灵）、阶跃星辰（[Step-Video](https://arxiv.org/abs/2502.10248)，300 亿参数）等与 OpenAI、NVIDIA、Google DeepMind 形成全球多极竞争格局，且正从视频生成向世界模型方向系统性转型。

6. **JEPA 理论路线的持续推进（2022–2025）**：Yann LeCun 的 [JEPA](https://openreview.net/forum?id=BZ5a1r-kVsf)（联合嵌入预测架构）提供了一条截然不同的理论路径——在抽象表征空间中预测而非在像素空间中生成；[V-JEPA 2](https://arxiv.org/abs/2506.09985) 实现理解、预测与规划的统一，V-JEPA 2-AC 将其扩展到具身控制。

**当前最活跃的五个交叉前沿：**
1. **世界模型 × 具身 AI** → 机器人在想象中学习（DreamZero、World-Gymnast、FLARE、UWM）← **2026 年最活跃方向**
2. **世界模型 × 视频生成** → 世界模拟器（Sora 2、Genie 3、Cosmos、Causal-Forcing）
3. **世界模型 × 自动驾驶** → 仿真数据 + 端到端规划（GAIA-3、Vista、Cosmos-Drive-Dreams）
4. **世界模型 × LLM Agent** → Agent 规划与推理（Affordances、WebDreamer、WALL-E 2.0）
5. **世界模型 × 3D/4D 生成** → 可交互虚拟世界（HunyuanWorld、WorldGen、OmniWorld）

---


# 什么是世界模型？

### 2.1 定义

World Model（世界模型）是一种 AI 系统，它学习对环境的内部表示，能够模拟环境在给定动作下的演变过程，从而使智能体通过"想象未来"进行规划，而非仅靠即时感知做出反应。

核心思想可追溯至认知科学家 Kenneth Craik（1943）在《The Nature of Explanation》中提出的"心智模型"概念——人脑构建外部世界的"小比例模型"来预测事件。在 AI 领域，这一思想被重新定义为：**学习环境的压缩时空表示，用于预测、规划和决策**。

具体而言，世界模型能够：

1. **编码当前状态**：将高维观测（如像素、点云、传感器数据）压缩为紧凑的内部表征
2. **预测未来状态**：给定当前状态和行动，预测下一个状态（即 $P(s_{t+1} | s_t, a_t)$）
3. **模拟后果**：通过迭代预测，在"想象"中评估不同行动序列的长期影响
4. **支持规划**：利用上述能力在内部模拟中搜索最优策略，而无需与真实环境交互

### 2.2 核心数学框架

世界模型的核心可以形式化为一个状态转移函数的学习问题：

$$\hat{s}_{t+1} = f_\theta(s_t, a_t)$$

其中 $s_t$ 是时刻 $t$ 的状态表示，$a_t$ 是行动，$f_\theta$ 是参数化的世界模型。

在更一般的概率框架下：

$$p_\theta(s_{t+1} | s_t, a_t)$$

世界模型学习的是状态转移的条件分布。在实践中，根据预测空间和建模方式的不同，这一框架衍生出多种变体：

| 范式 | 预测目标 | 代表方法 |
|------|---------|---------|
| **显式动力学模型** | 预测潜在状态向量 $s_{t+1}$ | Dreamer 系列（RSSM）、TD-MPC2 |
| **生成式模型** | 预测像素/token $o_{t+1}$ | Sora、Genie、DIAMOND |
| **联合嵌入预测** | 预测表征向量 $z_{t+1}$（丢弃不可预测细节） | JEPA（I-JEPA、V-JEPA） |
| **隐式世界模型** | 不显式预测，通过辅助损失对齐未来表征 | FLARE、TD-MPC |

### 2.3 世界模型 vs. 相关概念

| 概念 | 区别 |
|------|------|
| **视频生成模型** | 生成视觉上逼真的视频，但不一定理解物理规律或支持行动控制。关键争议见 [From Generative Engines to Actionable Simulators](https://arxiv.org/abs/2601.15533) |
| **模拟器（Simulator）** | 基于手工编码的物理/规则引擎；世界模型是从数据中学习的 |
| **数字孪生** | 特定物理系统的精确复制；世界模型是通用的、可泛化的 |
| **语言模型** | 在文本空间中建模序列分布，可能隐含地学习了某些世界知识（参见 [Othello-GPT](https://arxiv.org/abs/2210.13382)、[LLM Represent Space and Time](https://arxiv.org/abs/2310.02207)） |

### 2.4 为什么世界模型如此重要？

- **样本效率**：在想象中训练策略，大幅减少真实环境交互次数（DreamerV3 用想象训练首次从零收集 Minecraft 钻石）
- **安全性**：在内部模拟中测试危险场景（如自动驾驶碰撞、机器人失控），无需真实试错
- **泛化能力**：一个好的世界模型可以生成无限多样的训练场景（[Cosmos-Drive-Dreams](https://arxiv.org/abs/2506.09042) 用世界模型生成合成驾驶数据）
- **通向 AGI 的关键路径**：LeCun 认为世界模型是实现自主机器智能的核心组件；[DreamZero](https://arxiv.org/abs/2602.15922) 证明世界模型本身就可以作为零样本策略

---


# 发展历程与里程碑时间线

### 3.1 早期奠基（1943–2017）

| 年份 | 工作 | 作者/机构 | 贡献 |
|------|------|----------|------|
| 1943 | *The Nature of Explanation* | Kenneth Craik | **认知科学奠基**——首次提出"心智模型"概念：人脑构建外部世界的"小比例模型"来预测事件 |
| 1989 | *Making the World Differentiable* | Schmidhuber | **AI 奠基之作**——首次提出用 RNN 学习环境模型，使控制器可以通过世界模型的梯度学习 |
| 2011 | [PILCO](https://arxiv.org/abs/1102.0283) | Deisenroth & Rasmussen | 用高斯过程建模环境动力学，实现高样本效率的模型基强化学习 |
| 2015 | *On Learning to Think* | Schmidhuber | 统一框架：RNN 同时充当世界模型和控制器，提出"在想象中思考"的范式 |
| 2016 | [The Predictron](https://arxiv.org/abs/1612.08810) | Silver 等（DeepMind） | 将学习到的 MDP 模型集成到价值函数预测中 |
| 2017 | [Imagination-Augmented Agents](https://arxiv.org/abs/1707.06203) (I2A) | Weber 等（DeepMind） | 利用学习到的环境模型增强无模型智能体的想象力 |

### 3.2 经典世界模型时代（2018–2022）

| 年份 | 工作 | 作者/机构 | 贡献 |
|------|------|----------|------|
| 2018 | [**World Models**](https://arxiv.org/abs/1803.10122) ⭐ | Ha & Schmidhuber（Google Brain / NNAISENSE） | **里程碑论文**：提出 VAE + MDN-RNN + Controller (V-M-C) 三组件架构；首次实现在学习到的"梦境"中训练策略并迁移到真实环境 |
| 2019 | [PlaNet](https://arxiv.org/abs/1811.04551) | Hafner 等（DeepMind） | 纯粹在潜在空间中进行规划的模型基 RL，无需重建观测 |
| 2020 | [DreamerV1](https://arxiv.org/abs/1912.01603) | Hafner 等 | 在学习到的世界模型中使用 Actor-Critic 进行行为学习 |
| 2020 | [**MuZero**](https://arxiv.org/abs/1911.08265) | Schrittwieser 等（DeepMind） | **Nature 2020**：学习模型无需环境规则，统治棋类与 Atari，验证世界模型+规划搜索的威力 |
| 2021 | [DreamerV2](https://arxiv.org/abs/2010.02193) | Hafner 等 | 引入离散表征（categorical representations），在 Atari 上达到人类水平 |
| 2022 | [**LeCun 立场论文**](https://openreview.net/forum?id=BZ5a1r-kVsf) ⭐ | Yann LeCun（Meta AI） | 提出 **JEPA** 架构作为世界模型的核心范式：在抽象表征空间中预测，而非在像素空间中生成 |
| 2022 | [MILE](https://arxiv.org/abs/2210.07729) | Hu 等 | **NeurIPS 2022**：首个基于模型的模仿学习用于城市自动驾驶 |

### 3.3 基础世界模型时代（2023–2026）

| 日期 | 工作 | 机构 | 贡献 |
|------|------|------|------|
| 2023.01 | [**I-JEPA**](https://arxiv.org/abs/2301.08243) | Meta AI | **ICCV 2023**：图像 JEPA——通过预测遮蔽区域的表征（而非像素）实现自监督学习 |
| 2023.01 | [**DreamerV3**](https://arxiv.org/abs/2301.04104) ⭐ | Hafner 等 | 通用世界模型 RL 智能体：单一配置跨 150+ 任务；首个在 Minecraft 中从零收集钻石的算法；**2025 年发表于 Nature** |
| 2023.01 | [**Othello-GPT**](https://arxiv.org/abs/2210.13382) ⭐ | Li 等 | **ICLR 2023 Top 5%**：GPT 在棋步序列上训练后涌现出因果性棋盘状态表征——序列模型内部隐含世界模型的里程碑证据 |
| 2023.02 | [**IRIS**](https://arxiv.org/abs/2209.00588) | Micheli 等 | **ICLR 2023 Top 5%**：Transformer 作为采样高效世界模型，离散 token 化方法影响了 Genie 系列 |
| 2023.05 | [**RAP**](https://arxiv.org/abs/2305.14992) | Hao 等 | **EMNLP 2023**：将 LLM 同时用作世界模型和智能体，结合 MCTS 实现规划。LLaMA-33B+RAP 超越 GPT-4+CoT 33% |
| 2023.09 | [**GAIA-1**](https://arxiv.org/abs/2309.17080) ⭐ | Wayve | 90 亿参数自动驾驶世界模型；证明视频世界模型遵循类似 LLM 的幂律缩放定律 |
| 2023.10 | [**UniSim**](https://arxiv.org/abs/2310.06114) ⭐ | DeepMind / Berkeley / MIT | 通用交互式真实世界模拟器；训练的 RL 和 VL 策略可零样本部署到真实世界 |
| 2023.10 | [**TD-MPC2**](https://arxiv.org/abs/2310.16828) | UC San Diego | **ICLR 2024**：隐式世界模型；单个 3.17 亿参数智能体跨 80 个任务 |
| 2023.10 | [LLM 表征空间与时间](https://arxiv.org/abs/2310.02207) | Gurnee & Tegmark（MIT） | Llama-2 内部学习到了空间坐标和时间信息的线性表征 |
| 2023.11 | [**Copilot4D**](https://arxiv.org/abs/2311.01017) | Waabi / 多伦多大学 | **ICLR 2024**：LiDAR 点云世界模型（VQVAE + 离散扩散），比 SOTA 提升 65%+ |
| 2023.12 | [**LAW 框架**](https://arxiv.org/abs/2312.05230) | Hu & Shu | **NeurIPS 2023 Tutorial**：语言模型(L)、智能体模型(A)、世界模型(W) 三位一体框架 |
| 2024.01 | [WorldDreamer](https://arxiv.org/abs/2401.09985) | 清华 / GigaAI | 首个通用世界模型：通过遮蔽 token 预测统一文本到视频、图像到视频、视频编辑 |
| 2024.02 | [**Sora**](https://openai.com/index/video-generation-models-as-world-simulators/) ⭐ | OpenAI | 扩散 Transformer + 时空 patch：涌现出 3D 一致性、物体持久性、物理交互能力。明确提出"视频生成模型即世界模拟器" |
| 2024.02 | [**Genie 1**](https://arxiv.org/abs/2402.15391) ⭐ | Google DeepMind | 110 亿参数交互式环境生成器：从无标注视频中无监督学习，自动发现潜在动作空间 |
| 2024.02 | [**V-JEPA**](https://arxiv.org/abs/2404.08471) | Meta AI | 视频 JEPA：在抽象空间预测遮蔽视频区域，训练效率提升 1.5-6 倍 |
| 2024.05 | [**DIAMOND**](https://arxiv.org/abs/2405.12399) ⭐ | Alonso 等 | **NeurIPS 2024 Spotlight**：首个基于扩散的世界模型用于 RL，Atari 100k 达到 1.46 HNS 新 SOTA |
| 2024.05 | [**Vista**](https://arxiv.org/abs/2405.17398) | HKUST / 图宾根大学 | **NeurIPS 2024**：驾驶世界模型 SOTA，FID 改善 55%，首次实现自监督驾驶奖励 |
| 2024.08 | [**GameNGen**](https://arxiv.org/abs/2408.14837) ⭐ | Google | **ICLR 2025**：扩散模型作为实时游戏引擎，在 DOOM 中实现 20fps 实时交互 |
| 2024.08 | [**CogVideoX**](https://arxiv.org/abs/2408.06072) | 智谱 AI / 清华 | **ICLR 2025**：3D VAE + Expert DiT，首个被顶会录取的视频生成模型之一 |
| 2024.10 | [**Oasis**](https://oasis-model.github.io/) ⭐ | Decart / Etched | 纯 Transformer 实时开放世界模拟（类 Minecraft），20fps，500M 模型开源 |
| 2024.12 | [**Genie 2**](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/) ⭐ | Google DeepMind | 基础世界模型：从单张图片生成丰富 3D 世界，含 NPC、物理、最长 1 分钟一致性 |
| 2024.12 | [**HunyuanVideo**](https://arxiv.org/abs/2412.03603) | 腾讯 | 130 亿+ 参数，最大开源视频模型；双流→单流 DiT + MLLM 编码器 |
| 2025.01 | [**NVIDIA Cosmos**](https://arxiv.org/abs/2501.03575) ⭐ | NVIDIA | 首个开源世界基础模型平台：13 个代码仓库，完整物理 AI 管线（Apache-2.0） |
| 2025.01 | [**VideoWorld**](https://arxiv.org/abs/2501.09781) ⭐ | Ren 等 | 纯视频学习知识：3 亿参数模型仅从视频达到围棋五段水平，无需搜索或奖励 |
| 2025.02 | [**Step-Video-T2V**](https://arxiv.org/abs/2502.10248) | 阶跃星辰 | 300 亿参数，首创 Video-DPO 对齐技术 |
| 2025.03 | [**Wan2.1**](https://arxiv.org/abs/2503.20314) | 阿里巴巴 | 14B 参数，VBench 83.7%，Wan-VAE 支持无限长度 1080P（15.4k ⭐） |
| 2025.03 | [**GAIA-2**](https://arxiv.org/abs/2503.20523) | Wayve | 可控多视角生成世界模型 |
| 2025.04 | [**SkyReels-V2**](https://arxiv.org/abs/2504.13074) | 昆仑万维 | **首个开源 Diffusion Forcing 模型**，VBench SOTA 83.9%，无限长度视频 |
| 2025.05 | [**FLARE**](https://arxiv.org/abs/2505.15659) | NVIDIA | 隐式世界模型用于机器人学习，集成 Isaac GR00T，近零推理开销提升 26% |
| 2025.06 | [**V-JEPA 2**](https://arxiv.org/abs/2506.09985) | Meta AI | 自监督视频模型实现理解、预测和规划的统一；V-JEPA 2-AC 扩展到具身控制 |
| 2025.06 | [**Cosmos-Drive-Dreams**](https://arxiv.org/abs/2506.09042) | NVIDIA | 可扩展合成驾驶数据生成 |
| 2025.07 | [**HunyuanWorld 1.0**](https://arxiv.org/abs/2507.21809) | 腾讯 | 从文字/图像生成沉浸式可探索交互 3D 世界 |
| 2025.08 | [**Genie 3**](https://deepmind.google/blog/genie-3-a-new-frontier-for-world-models/) ⭐ | Google DeepMind | **首个实时交互世界模型**：24fps/720p 生成逼真可交互 3D 世界 |
| 2025.09 | [**Sora 2**](https://openai.com/blog/sora-2) ⭐ | OpenAI | 物理精度大幅提升，多镜头复杂指令，逼真音频生成，iOS 应用 |
| 2025.09 | [**Dreamer4**](https://arxiv.org/abs/2509.24527) | Hafner | 可扩展的世界模型内训练智能体 |
| 2025.10 | [**GAIA-3**](https://wayve.ai/thinking/gaia-3/) | Wayve | 15B 参数驾驶世界模型，9 国 3 大洲数据，5× 计算 10× 数据 |
| 2025.10 | [**Aether**](https://arxiv.org/abs/2503.18945) | InternRobotics | **ICCV 2025 杰出论文 & RIWM Outstanding Paper**：几何感知的统一世界建模 |
| 2025.11 | [**SIMA 2**](https://deepmind.google/blog/sima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds/) | Google DeepMind | Gemini 驱动的通用 3D 虚拟世界智能体，适配 Genie 3 世界 |
| 2026.02 | [**DreamZero**](https://arxiv.org/abs/2602.15922) | Google DeepMind | 14B 视频扩散世界动作模型，7Hz 实时闭环控制，泛化比 SOTA VLA 提升 2×+ |
| 2026.02 | [**World-Gymnast**](https://arxiv.org/abs/2602.02454) | Stanford | 在视频世界模型中对 VLA 做 RL 微调，比 SFT 提升 18× |
| 2026.02 | 研究爆发期 | — | 仅 2026 年前两月，arXiv 上 World Model 相关论文已超百篇 |

---


# 方向一：基于强化学习的世界模型

### 4.1 方向概述

这是世界模型最经典的研究路线。核心思想是：**在潜在空间中学习环境动力学模型，然后在"想象"的轨迹中训练策略**，从而大幅提升样本效率。从 Schmidhuber（1989）到 Dreamer4（2025），这条路线持续 36 年不断演进，始终是世界模型研究最坚实的理论基础。

### 4.2 关键论文深度解析

#### 4.2.1 World Models（Ha & Schmidhuber, 2018）— 奠基之作

| 项目 | 内容 |
|------|------|
| **论文** | [*Recurrent World Models Facilitate Policy Evolution*](https://arxiv.org/abs/1803.10122) |
| **作者** | David Ha（Google Brain），Jürgen Schmidhuber（NNAISENSE / IDSIA） |
| **发表** | NeurIPS 2018 |

**三组件架构（V-M-C）：**

| 组件 | 模型 | 功能 | 参数量 |
|------|------|------|--------|
| **V（视觉）** | 变分自编码器 (VAE) | 将 64×64 像素帧压缩为潜在向量 $z_t \in \mathbb{R}^{32}$ | ~430 万 |
| **M（记忆）** | MDN-RNN（LSTM + 混合密度网络） | 预测未来潜在状态：$P(z_{t+1} \mid a_t, z_t, h_t)$ | ~42-170 万 |
| **C（控制器）** | 单层线性层 | 将 $[z_t, h_t]$ 映射到动作 $a_t$；用 CMA-ES 进化策略训练 | **867-1088** |

**核心创新：**
- **梦境训练（Dream Training）**：智能体完全在潜在空间的"梦境"中训练，无需与真实环境交互
- **温度控制（$\tau$）**：调节世界模型的随机性，防止智能体利用模型缺陷
- **极简控制器**：将复杂性移入 V 和 M，使 C 极度精简（仅 ~1000 参数），可用进化策略训练

**成果**：CarRacing-v0 得分 906 ± 21（首次解决该任务），VizDoom Take Cover 得分 1092 ± 556（完全在梦境中训练）。

#### 4.2.2 Dreamer 系列技术演进（2020–2025）

| 版本 | 年份 | 论文 | 关键改进 | 重要成果 |
|------|------|------|---------|---------|
| **DreamerV1** | 2020 | [arXiv:1912.01603](https://arxiv.org/abs/1912.01603) | 在学习到的世界模型中使用 Actor-Critic 学习行为，替代进化策略 | 多个连续控制任务 SOTA |
| **DreamerV2** | 2021 | [arXiv:2010.02193](https://arxiv.org/abs/2010.02193) | 引入离散表征（categorical representations），从连续高斯切换到 32×32 one-hot 向量 | Atari 达到人类水平 |
| **DreamerV3** ⭐ | 2023 | [arXiv:2301.04104](https://arxiv.org/abs/2301.04104) | 基于 symlog 预测、percentile scaling 等归一化技术，**单一配置跨所有域** | 150+ 任务最优；Minecraft 首个从零挖钻石；**Nature 2025** |
| **Dreamer4** | 2025 | [arXiv:2509.24527](https://arxiv.org/abs/2509.24527) | 可扩展的世界模型内训练智能体，突破 DreamerV3 的规模限制 | 扩展性验证 |

**DreamerV3 核心方法**：学习环境的世界模型（RSSM），然后在想象轨迹中训练 actor-critic。关键在于使用 categorical representations 和一系列归一化技术，使同一套超参数可以跨越 Atari、DMControl、Minecraft 等完全不同的任务域。

**里程碑意义**：DreamerV3 发表于 **Nature (2025)**，标志着世界模型 RL 方法获得了最高级别的学术认可。其在 Minecraft 中从零收集钻石（需要长达 20,000+ 步的远视探索）是 AI 领域的标志性挑战。

#### 4.2.3 TD-MPC2 — 隐式世界模型

| 项目 | 内容 |
|------|------|
| **论文** | [*TD-MPC2: Scalable, Robust World Models for Continuous Control*](https://arxiv.org/abs/2310.16828) |
| **发表** | ICLR 2024 |
| **代码** | [github.com/nicklashansen/tdmpc2](https://github.com/nicklashansen/tdmpc2)（758 ⭐） |

**核心方法**：在学习到的**隐式（无解码器）世界模型**的潜在空间中进行局部轨迹优化（模型预测控制）。与 Dreamer 系列不同，TD-MPC2 不重建观测——完全在潜在空间中规划。

**关键发现**：
- 单一 3.17 亿参数智能体可跨 80 个任务
- 智能体能力随模型和数据规模增长，验证了世界模型的缩放定律
- 104 个在线 RL 任务中表现一致

#### 4.2.4 DIAMOND — 扩散世界模型

| 项目 | 内容 |
|------|------|
| **论文** | [*Diffusion for World Modeling: Visual Details Matter in Atari*](https://arxiv.org/abs/2405.12399) |
| **发表** | NeurIPS 2024 Spotlight |
| **代码** | [github.com/eloialonso/diamond](https://github.com/eloialonso/diamond)（2k ⭐） |

**核心洞察**：离散 token 化会**丢弃对 RL 性能至关重要的视觉细节**。DIAMOND 使用连续扩散替代离散 token 进行环境模拟。

**成果**：Atari 100k 平均人类归一化得分 **1.46**——世界模型训练智能体的新 SOTA。还可作为 CS:GO 的交互式神经游戏引擎。

### 4.3 前沿论文全景

| 论文 | 会议/时间 | 核心贡献 | 链接 |
|------|----------|---------|------|
| [MuZero](https://arxiv.org/abs/1911.08265) | Nature 2020 | 学习的模型用于搜索规划，无需环境规则，统治棋类与 Atari | [arXiv](https://arxiv.org/abs/1911.08265) |
| [IRIS](https://arxiv.org/abs/2209.00588) | ICLR 2023 Top 5% | Transformer 世界模型，将动力学学习转化为序列建模问题 | [GitHub](https://github.com/eloialonso/iris) |
| [STORM](https://arxiv.org/abs/2310.09615) | 2023 | Transformer+随机 VAE，单卡 4.3h 训练达 126.7% 人类水平 | [arXiv](https://arxiv.org/abs/2310.09615) |
| [SafeDreamer](https://arxiv.org/abs/2307.07176) | ICLR 2024 | 安全约束融入 Dreamer 框架，近零代价性能 | [arXiv](https://arxiv.org/abs/2307.07176) |
| [Diffusion Forcing](https://arxiv.org/abs/2407.01392) | NeurIPS 2024 | 独立逐 token 噪声训练范式，统一自回归与全序列扩散 | [arXiv](https://arxiv.org/abs/2407.01392) |
| [MoSim](https://arxiv.org/abs/2504.07095) | CVPR 2025 | 推动 RL 中世界模型的极限 | [arXiv](https://arxiv.org/abs/2504.07095) |
| [Dreamer4](https://arxiv.org/abs/2509.24527) | 2025.09 | 可扩展的世界模型内训练智能体 | [arXiv](https://arxiv.org/abs/2509.24527) |
| WIMLE | ICLR 2026 | 基于 IMLE 的不确定性感知世界模型 | — |
| Horizon Imagination | ICLR 2026 | 扩散世界模型中的高效策略训练 | — |
| [Mixture-of-World Models](https://arxiv.org/abs/2602.01270) | 2026.02 | 混合世界模型：模块化潜在动态扩展多任务 RL | [arXiv](https://arxiv.org/abs/2602.01270) |
| [ObjectZero](https://arxiv.org/abs/2601.07823) | 2026.01 | 对象中心世界模型结合蒙特卡洛树搜索 | [arXiv](https://arxiv.org/abs/2601.07823) |
| [Event-Aware WM](https://arxiv.org/abs/2601.14134) | 2026.01 | 事件感知世界模型提升 MBRL 样本效率 | [arXiv](https://arxiv.org/abs/2601.14134) |

### 4.4 方向小结

| 特征 | 描述 |
|------|------|
| **优势** | 样本效率高、理论清晰、可直接用于策略学习 |
| **局限** | 通常限于特定任务域，难以处理极复杂的开放世界 |
| **趋势** | 从 RSSM → Transformer → 扩散模型演进；正在与大规模视频生成方法融合，走向通用化；安全约束日益受到关注 |
| **代表方法** | Dreamer 系列、TD-MPC2、DIAMOND、IRIS、MuZero |

---


# 方向二：视频生成作为世界模拟

### 5.1 方向概述

2024 年 Sora 的发布标志着一个范式转换：**大规模视频生成模型本身就是世界模拟器**。这一路线主张，在互联网规模的视频数据上训练的扩散模型会涌现出对 3D 空间、物理规律、物体交互的理解。然而，这一命题也引发了最激烈的学术争议——"视频生成 ≠ 世界理解"的批评方指出，视觉逼真度是世界理解的不可靠代理（[From Generative Engines to Actionable Simulators](https://arxiv.org/abs/2601.15533)）。

### 5.2 关键论文深度解析

#### 5.2.1 Sora 系列技术演进（2024–2025）

| 维度 | **Sora 1**（2024.02） | **Sora 2**（2025.09） |
|------|------|------|
| **架构** | 扩散 Transformer (DiT) + 时空 patch | 架构升级（细节未公开） |
| **核心创新** | 时空 patch 表示；可变分辨率/时长/宽高比；DALL·E 3 重标注 | 物理精度大幅提升（正确模拟碰撞反弹等）；多镜头复杂指令；逼真音频生成 |
| **涌现能力** | 3D 一致性、物体持久性、基础物理交互、零样本 Minecraft 模拟 | 精确物理模拟、"客串"功能、iOS 原生应用 |
| **局限** | 无法准确模拟玻璃破碎等复杂物理；空间混淆（左/右） | 部分复杂场景仍有伪影 |
| **开放性** | 闭源，仅技术报告 | 闭源，API + iOS 应用 |
| **关键意义** | 将"视频生成 = 世界模拟"推向学术前台 | 证明缩放效应可持续改善物理精度 |

**Sora 1 的学术影响**：将"视频生成模型是否可以成为世界模拟器"从学术假设提升为领域中心议题。缩放效应的证据（1×、4×、32× 计算量下质量显著提升）表明这是一条可行的路径。

#### 5.2.2 VideoWorld — 纯视频学习知识

| 项目 | 内容 |
|------|------|
| **论文** | [*VideoWorld: Exploring Knowledge Learning from Unlabeled Videos*](https://arxiv.org/abs/2501.09781) |
| **日期** | 2025.01 |
| **开源** | 代码、数据、模型全部开源 |

**核心发现**：
1. **纯视频训练**提供了足够信息来学习规则、推理和规划能力
2. 引入**潜在动力学模型（Latent Dynamics Model, LDM）**作为知识获取的核心组件
3. 仅 3 亿参数模型**从视频达到围棋五段水平**，无需搜索算法或奖励机制
4. 在机器人任务（CALVIN、RLBench）中有效学习多样控制操作

**意义**：为"视频预测模型就是真正的世界模型"提供了最强证据——它们能从纯视觉观察中学习规则、物理和策略。

### 5.3 前沿论文全景

| 论文 | 机构/时间 | 核心贡献 | 链接 |
|------|----------|---------|------|
| [WorldDreamer](https://arxiv.org/abs/2401.09985) | 清华/GigaAI, 2024.01 | 首个通用世界模型：遮蔽 token 预测统一 T2V/I2V/编辑 | [arXiv](https://arxiv.org/abs/2401.09985) |
| [LVM](https://arxiv.org/abs/2312.00785) | UC Berkeley, 2023.12 | 大视觉模型：4200 亿视觉 token 上 next-token 预测 | [arXiv](https://arxiv.org/abs/2312.00785) |
| [**CogVideoX**](https://arxiv.org/abs/2408.06072) | 智谱 AI/清华, ICLR 2025 | 3D VAE + Expert DiT，10 秒连续视频，12.4k ⭐ | [GitHub](https://github.com/THUDM/CogVideo) |
| [**HunyuanVideo**](https://arxiv.org/abs/2412.03603) | 腾讯, 2024.12 | 130 亿参数，双流→单流 DiT + MLLM 编码器 | [GitHub](https://github.com/Tencent/HunyuanVideo) |
| [**Wan2.1**](https://arxiv.org/abs/2503.20314) | 阿里巴巴, 2025.03 | 14B 参数，VBench 83.7%，Wan-VAE 支持无限长度 1080P | [GitHub](https://github.com/Wan-Video/Wan2.1)（15.4k ⭐） |
| [**Step-Video-T2V**](https://arxiv.org/abs/2502.10248) | 阶跃星辰, 2025.02 | 300 亿参数，首创 Video-DPO 对齐技术 | [GitHub](https://github.com/stepfun-ai/Step-Video-T2V) |
| [**SkyReels-V2**](https://arxiv.org/abs/2504.13074) | 昆仑万维, 2025.04 | **首个开源 Diffusion Forcing 模型**，VBench SOTA 83.9% | [GitHub](https://github.com/SkyworkAI/SkyReels-V2) |
| **Seedance 1.0→2.0** | 字节跳动 | 闭源；2.0 统一多模态架构，导演级控制 | 产品：即梦/豆包 |
| **可灵 Kling** | 快手 | 闭源；MVL 方法，中国市场占有率领先 | [klingai.com](https://klingai.com) |
| [AniSora](https://arxiv.org/abs/2412.10255) | 哔哩哔哩, 2024.12 | 首个动画视频生成系统，扩展到艺术/幻想世界模拟 | [GitHub](https://github.com/bilibili/Index-anisora) |
| [Open-Sora](https://github.com/hpcaitech/Open-Sora) | 2024–2025 | 最大规模 Sora 开源复现，110 亿参数 v2.0 仅 $200K 训练成本 | [GitHub](https://github.com/hpcaitech/Open-Sora)（28.6k ⭐） |
| [Open-Sora-Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan) | 北大, 2024 | 开源 Sora 复现，支持长视频生成 | [GitHub](https://github.com/PKU-YuanGroup/Open-Sora-Plan)（12.1k ⭐） |
| [VideoReward / Flow-DPO](https://arxiv.org/abs/2501.13918) | 2025.01 | 视频生成的 RLHF 对齐：多维度奖励模型 + 流模型 DPO | [arXiv](https://arxiv.org/abs/2501.13918) |
| [Causal-Forcing](https://arxiv.org/abs/2602.02214) | 清华 thu-ml, 2026.02 | 自回归扩散蒸馏用于高质量实时交互视频生成 | [GitHub](https://github.com/thu-ml/Causal-Forcing) |
| [Matrix-Game 2.0](https://arxiv.org/abs/2508.13009) | SkyworkAI, 2025 | 开源实时流式交互式世界模型 | [GitHub](https://github.com/SkyworkAI/Matrix-Game) |
| [LongVie 2](https://arxiv.org/abs/2512.13604) | 2025.12 | 多模态可控超长视频世界模型 | [arXiv](https://arxiv.org/abs/2512.13604) |
| [LIVE](https://arxiv.org/abs/2602.03747) | 2026.02 | 长时交互式视频世界建模 | [arXiv](https://arxiv.org/abs/2602.03747) |
| [VerseCrafter](https://arxiv.org/abs/2601.05138) | TencentARC, 2026.01 | 动态真实视频世界模型 + 4D 几何控制 | [arXiv](https://arxiv.org/abs/2601.05138) |
| [Astra](https://arxiv.org/abs/2512.08931) | 2025.12 | 自回归去噪的通用交互世界模型 | [arXiv](https://arxiv.org/abs/2512.08931) |

**视频基座模型架构趋同**：全球主要开源视频模型在架构上高度趋同——**DiT + 3D VAE + Flow Matching** 成为共识技术栈。差异化竞争集中在 VAE 设计（Wan-VAE 无限长度 vs CogVideoX 3D VAE）、后训练策略（Video-DPO / Diffusion Forcing / RL）和文本编码器（MLLM vs T5）上。

### 5.4 方向小结

| 特征 | 描述 |
|------|------|
| **优势** | 视觉丰富、可利用互联网规模数据、涌现能力显著 |
| **局限** | 物理精度不足、缺乏动作条件化、计算成本极高 |
| **核心争议** | "视频生成 ≠ 世界理解"——视觉逼真度是世界理解的不可靠代理 |
| **趋势** | 从被动生成走向动作可控的交互式模拟；实时性成为新要求；RLHF 对齐技术迁入 |
| **代表方法** | Sora 系列、VideoWorld、CogVideoX、HunyuanVideo、Wan2.1、SkyReels-V2 |

---


# 方向三：自动驾驶世界模型

### 6.1 方向概述

自动驾驶是世界模型最直接的应用场景之一，也是论文最密集的领域。核心需求是：**在虚拟环境中安全地测试和训练策略，然后零样本迁移到真实世界**。这个方向的世界模型需要处理多模态传感器数据（相机、LiDAR）、复杂交通规则和安全约束。

### 6.2 关键论文深度解析

#### 6.2.1 GAIA 系列技术演进（2023–2025）

| 维度 | **GAIA-1**（2023.09） | **GAIA-2**（2025.03） | **GAIA-3**（2025.10） |
|------|------|------|------|
| **参数** | 90 亿 | 未公开 | **15B** |
| **架构** | 多模态编码器(视频+文本+动作) → 自回归 Transformer(65 亿) → 视频扩散解码器(26 亿) | 可控多视角生成世界模型 | 潜在扩散世界模型 |
| **训练数据** | 4700h 伦敦驾驶 | 多视角数据 | **9 国 3 大洲**数据，10× 数据量 |
| **计算** | — | — | **5× 计算** |
| **关键发现** | 视频世界模型遵循类似 LLM 的幂律缩放定律 | 精细多视角控制 | 大规模跨地域泛化 |
| **链接** | [arXiv](https://arxiv.org/abs/2309.17080) | [arXiv](https://arxiv.org/abs/2503.20523) | [Blog](https://wayve.ai/thinking/gaia-3/) |

**GAIA-1 核心贡献**：首次证明视频世界模型遵循缩放定律——性能随计算量可预测地提升。通过动作输入控制自车行为，通过文本提示控制场景特征（天气、时间、交通），实现精细可控生成。

#### 6.2.2 UniSim — 通用真实世界模拟器

| 项目 | 内容 |
|------|------|
| **论文** | [*Learning Interactive Real-World Simulators*](https://arxiv.org/abs/2310.06114) |
| **机构** | Google DeepMind / UC Berkeley / MIT |
| **发表** | 2023.10 |

**核心洞察**：不同的自然数据集在不同维度上很丰富（图像数据中物体丰富、机器人数据中动作密集、导航数据中运动多样）。通过精心**编排多样数据集**，UniSim 可以模拟高级指令和低级控制的视觉结果。

**里程碑意义**：在 UniSim 中训练的 RL 策略和视觉语言策略都可以**零样本部署到真实世界**——有效地通过学习到的世界模型弥合了 sim-to-real 鸿沟。

#### 6.2.3 Vista — 驾驶世界模型 SOTA

| 项目 | 内容 |
|------|------|
| **论文** | [*Vista: A Generalizable Driving World Model*](https://arxiv.org/abs/2405.17398) |
| **发表** | NeurIPS 2024 |
| **代码** | [github.com/OpenDriveLab/Vista](https://github.com/OpenDriveLab/Vista)（855 ⭐） |

**成果**：驾驶世界模型 SOTA，FID 改善 55%、FVD 改善 27%；首次实现自监督驾驶奖励评估。

### 6.3 前沿论文全景

| 论文 | 会议/时间 | 核心贡献 | 链接 |
|------|----------|---------|------|
| [MILE](https://arxiv.org/abs/2210.07729) | NeurIPS 2022 | 首个基于模型的模仿学习用于城市驾驶 | [arXiv](https://arxiv.org/abs/2210.07729) |
| [DriveDreamer](https://arxiv.org/abs/2309.09777) | 2023.09 | 首个完全基于真实驾驶数据的世界模型 | [arXiv](https://arxiv.org/abs/2309.09777) |
| [Copilot4D](https://arxiv.org/abs/2311.01017) | ICLR 2024 | LiDAR 点云世界模型（VQVAE + 离散扩散），比 SOTA 降低 65%+ | [arXiv](https://arxiv.org/abs/2311.01017) |
| [OccWorld](https://arxiv.org/abs/2311.16038) | ECCV 2024 | 3D Occupancy 世界模型 | [arXiv](https://arxiv.org/abs/2311.16038) |
| [ViDAR](https://arxiv.org/abs/2312.17655) | CVPR 2024 Highlight | 视觉点云预测预训练 | [arXiv](https://arxiv.org/abs/2312.17655) |
| [GenAD](https://arxiv.org/abs/2402.11502) | CVPR 2024 Highlight | 2000+ 小时网络驾驶视频，零样本跨数据集泛化 | [arXiv](https://arxiv.org/abs/2402.11502) |
| [DriveDreamer-2](https://arxiv.org/abs/2403.06845) | 2024.03 | LLM 增强的驾驶视频生成 | [arXiv](https://arxiv.org/abs/2403.06845) |
| [DrivingGPT](https://arxiv.org/abs/2412.18607) | 2024.12 | 统一驾驶世界建模与规划 | [arXiv](https://arxiv.org/abs/2412.18607) |
| [Doe-1](https://arxiv.org/abs/2412.09627) | 2024.12 | 闭环自动驾驶 + 大型世界模型 | [arXiv](https://arxiv.org/abs/2412.09627) |
| [Cosmos-Drive-Dreams](https://arxiv.org/abs/2506.09042) | NVIDIA, 2025.06 | 可扩展合成驾驶数据生成 | [arXiv](https://arxiv.org/abs/2506.09042) |
| [FutureSightDrive](https://arxiv.org/abs/2505.17685) | NeurIPS 2025 Spotlight | 时空 CoT 视觉思考 | [arXiv](https://arxiv.org/abs/2505.17685) |
| [AD-R1](https://arxiv.org/abs/2511.20325) | 2025.11 | 闭环 RL 端到端驾驶 + 公正世界模型 | [arXiv](https://arxiv.org/abs/2511.20325) |
| [WorldRFT](https://arxiv.org/abs/2512.19133) | AAAI 2026 | 潜在世界模型规划 + RL 微调 | [arXiv](https://arxiv.org/abs/2512.19133) |
| [DriveWorld-VLA](https://arxiv.org/abs/2602.06521) | 2026.02 | 统一潜在空间世界建模 + VLA | [arXiv](https://arxiv.org/abs/2602.06521) |
| [ResWorld](https://arxiv.org/abs/2602.10884) | 2026.02 | 时序残差世界模型用于端到端驾驶 | [arXiv](https://arxiv.org/abs/2602.10884) |

**核心子方向**：
- **4D Occupancy 预测**：OccWorld、GaussianWorld、SparseWorld、OccTENS
- **BEV 世界模型**：BEVWorld、Drive-OccWorld
- **仿真数据生成**：Cosmos-Drive-Dreams、SimGen、DriveDreamer-2
- **端到端驾驶决策**：Doe-1、DriveVLA-W0、Raw2Drive
- **LiDAR 世界模型**：Copilot4D、LiDARCrafter、LiSTAR、AD-L-JEPA

### 6.4 方向小结

| 特征 | 描述 |
|------|------|
| **优势** | 直接对接工业应用、安全关键场景测试、数据生成 |
| **局限** | 需要极高物理精度、多模态融合挑战、长尾场景覆盖 |
| **趋势** | 从相机到 LiDAR 到多模态融合；从任务特定到基础模型；缩放定律已验证 |
| **代表方法** | GAIA 系列、UniSim、Vista、GenAD、Copilot4D |

---


# 方向四：具身 AI 世界模型

### 7.1 方向概述

具身 AI 是 **2026 年世界模型研究最活跃的方向**（占全部论文约 30%）。核心目标是为机器人操作、导航、运动控制构建世界模型，实现在想象中学习和规划。2025–2026 年，**VLA（Vision-Language-Action）与世界模型的深度融合**成为最重要的技术趋势，多个重量级工作验证了"在世界模型中训练机器人策略"的可行性。

### 7.2 关键论文深度解析

#### 7.2.1 DreamZero — 世界模型即策略

| 项目 | 内容 |
|------|------|
| **论文** | [*DreamZero: World Action Models are Zero-shot Policies*](https://arxiv.org/abs/2602.15922) |
| **机构** | Google DeepMind |
| **日期** | 2026.02 |

**方法**：14B 参数视频扩散模型联合建模视频和动作（World Action Model, WAM），作为 zero-shot 策略直接输出机器人动作。

**核心突破**：
- **世界模型本身就是策略**——不需要额外的策略网络
- 7Hz 实时闭环控制
- 泛化能力比 SOTA VLA 提升 **2× 以上**
- 仅需 **30 分钟 play data** 即可跨具身形态迁移

#### 7.2.2 FLARE — 隐式世界模型

| 项目 | 内容 |
|------|------|
| **论文** | [*FLARE: Implicit World Models for Robotic Learning*](https://arxiv.org/abs/2505.15659) |
| **机构** | NVIDIA，集成 Isaac GR00T |
| **日期** | 2025.05 |

**核心洞见**：世界模型不一定要显式生成未来，只需让策略网络"隐式地预测未来"就能大幅提升性能。在扩散 Transformer 中添加少量 token，将特征与未来观测的潜在嵌入对齐。

**成果**：提升 **26%**，**近零推理开销**——是实用性最强的世界模型增强方法之一。

#### 7.2.3 V-JEPA 2-AC — 表征空间的具身控制

| 项目 | 内容 |
|------|------|
| **论文** | [*V-JEPA 2: Self-Supervised Video Model for Understanding, Prediction and Planning*](https://arxiv.org/abs/2506.09985) |
| **机构** | Meta AI |
| **日期** | 2025.06 |

V-JEPA 2 系列的 Action-Conditioned 变体将 JEPA 的表征空间预测扩展到具身控制场景。在表示空间（而非像素空间）进行预测和规划，实现了理解、预测与规划的统一。

### 7.3 VLA + World Model 融合：五大范式

VLA 与世界模型的融合是 2025–2026 年最重要的技术趋势。当前已形成五种成熟范式：

#### 范式 1：世界模型作为 RL 模拟器

在动作条件视频世界模型中对 VLA 做 RL 微调，VLM 提供奖励信号。

| 工作 | 方法 | 效果 |
|------|------|------|
| [**World-Gymnast**](https://arxiv.org/abs/2602.02454) | 视频 WM 中 RL 微调 VLA，VLM 奖励 | **比 SFT 提升 18×，比模拟器提升 2×** |
| [GigaBrain-0.5M](https://arxiv.org/abs/2602.12099) | RAMP（RL via World Model）优化 VLA | 操控任务提升约 30% |

#### 范式 2：VLA 与世界模型迭代共进化

VLA 策略与世界模型交替迭代训练，双方同时改进。

| 工作 | 方法 | 效果 |
|------|------|------|
| [**VLAW**](https://arxiv.org/abs/2602.12063) | VLA 与 WM 交替迭代训练 | 绝对性能提升 **+39.2%** |
| [World-VLA-Loop](https://arxiv.org/abs/2602.06508) | 视频 WM 与 VLA 闭环学习 | 双方同时改进 |

#### 范式 3：隐式世界模型增强 VLA

不显式生成未来，通过辅助损失对齐未来表征（仅训练时使用）。

| 工作 | 方法 | 效果 |
|------|------|------|
| [**FLARE**](https://arxiv.org/abs/2505.15659) | 扩散 Transformer + 少量预测 token | **提升 26%，近零推理开销** |
| [VLA-JEPA](https://arxiv.org/abs/2602.10098) | 潜在世界模型增强 VLA | 超越基线 |

#### 范式 4：统一世界-动作模型（WAM）

视频扩散骨干联合建模视频预测和动作预测。

| 工作 | 方法 | 效果 |
|------|------|------|
| [**DreamZero**](https://arxiv.org/abs/2602.15922) | 14B WAM 联合建模视频+动作 | **泛化 2×+，30min 数据跨具身迁移** |
| [UWM](https://arxiv.org/abs/2504.02792) | 统一 Transformer 中视频+动作扩散 | 支持有/无动作标注混合训练 |

#### 范式 5：潜在动作预训练

从互联网视频（无动作标签）中学习潜在动作表示，再微调到真实动作。

| 工作 | 方法 | 效果 |
|------|------|------|
| [**LAPA**](https://arxiv.org/abs/2410.11758) | VQ-VAE 从视频提取潜在动作 | **超越有真实动作标签训练的 SOTA VLA**（ICLR 2025） |
| [Motus](https://github.com/thu-ml/Motus) | 统一潜在动作世界模型 | 无需动作标签的通用预训练 |

### 7.4 前沿论文全景

| 论文 | 会议/时间 | 核心贡献 | 链接 |
|------|----------|---------|------|
| [DWL](https://arxiv.org/abs/2408.14472) | RSS 2024 最佳论文入围 | 去噪世界模型学习用于人形运动 | [arXiv](https://arxiv.org/abs/2408.14472) |
| [ManiGaussian](https://arxiv.org/abs/2403.08321) | 2024.03 | 高斯溅射用于多任务机器人操控 | [arXiv](https://arxiv.org/abs/2403.08321) |
| [AdaWorld](https://arxiv.org/abs/2503.18938) | 2025.03 | 带潜在动作的可适应世界模型 | [arXiv](https://arxiv.org/abs/2503.18938) |
| [Humanoid World Models](https://arxiv.org/abs/2506.01182) | 2025.06 | 开放世界人形机器人基础模型 | [arXiv](https://arxiv.org/abs/2506.01182) |
| [GWM](https://arxiv.org/abs/2508.17600) | ICCV 2025 | 可扩展高斯世界模型用于操控 | [arXiv](https://arxiv.org/abs/2508.17600) |
| [GigaBrain-0](https://arxiv.org/abs/2510.19430) | 2025.10 | 世界模型驱动的 VLA 策略学习 | [arXiv](https://arxiv.org/abs/2510.19430) |
| [PointWorld](https://arxiv.org/abs/2601.03782) | 2026.01 | 3D 点云世界模型用于真实世界机器人操控 | [arXiv](https://arxiv.org/abs/2601.03782) |
| [DreamDojo](https://arxiv.org/abs/2602.06949) | 2026.02 | 从大规模人类视频学习通用机器人世界模型 | [arXiv](https://arxiv.org/abs/2602.06949) |
| [RWM-U](https://arxiv.org/abs/2504.16680) | 2025.04 | 不确定性感知世界模型，真实四足/人形部署 | [arXiv](https://arxiv.org/abs/2504.16680) |
| [π₀](https://arxiv.org/abs/2410.24164) | RSS 2025 | Flow Matching VLA 基础模型，覆盖多种具身形态 | [arXiv](https://arxiv.org/abs/2410.24164) |
| [OpenVLA](https://arxiv.org/abs/2406.09246) | 2024.06 | 开源 7B VLA，7× 更少参数超越 RT-2-X | [arXiv](https://arxiv.org/abs/2406.09246) |

### 7.5 方向小结

| 特征 | 描述 |
|------|------|
| **优势** | 直接服务机器人部署、VLA+WM 融合趋势不可逆、互联网视频可作为免费训练数据 |
| **局限** | 长时域误差累积、实时推理（100Hz+）仍需攻克、跨形态迁移困难 |
| **趋势** | DreamZero 和 World-Gymnast 标志着"在世界模型中训练策略"从理论走向实践；3D 高斯溅射作为物理世界表示正在兴起 |
| **代表方法** | DreamZero、FLARE、World-Gymnast、UWM、V-JEPA 2-AC、LAPA |

---


# 方向五：LLM 作为与增强世界模型

### 8.1 方向概述

这一方向探索三个核心问题：（1）LLM 是否隐含地学习了世界模型？（2）如何将 LLM 显式地用作世界模型进行规划？（3）Agent + World Model 如何协同？

### 8.2 关键论文深度解析

#### 8.2.1 Othello-GPT — 序列模型中涌现世界表征

| 项目 | 内容 |
|------|------|
| **论文** | [*Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task*](https://arxiv.org/abs/2210.13382) |
| **发表** | **ICLR 2023, Notable Top 5%** |

**实验设计**：训练一个 GPT 变体预测黑白棋（Othello）的合法走法——模型对游戏规则完全无先验知识。

**关键发现**：
1. 使用**非线性探针**揭示模型内部涌现出了棋盘状态的表征
2. **干预实验**：修改内部表征会因果地改变预测的合法走法
3. 探针恢复棋盘状态的错误率仅 0.01%

**里程碑意义**：**序列模型在 next-token 预测中可以涌现出因果性世界模型**的最重要证据之一。

#### 8.2.2 RAP — 推理即规划

| 项目 | 内容 |
|------|------|
| **论文** | [*Reasoning with Language Model is Planning with World Model*](https://arxiv.org/abs/2305.14992) |
| **发表** | **EMNLP 2023** |

**方法**：将 LLM 同时用作**世界模型**（预测状态转移）和**智能体**（选择行动），使用**蒙特卡洛树搜索（MCTS）**在 LLM 生成的世界状态上进行战略性探索。

**成果**：LLaMA-33B + RAP 在计划生成任务中**超越 GPT-4 + CoT 33%**。

### 8.3 前沿论文全景

| 论文 | 会议/时间 | 核心贡献 | 链接 |
|------|----------|---------|------|
| [LLM 表征空间与时间](https://arxiv.org/abs/2310.02207) | 2023.10 | Llama-2 学习到空间坐标和时间信息的线性表征 | [arXiv](https://arxiv.org/abs/2310.02207) |
| [LAW 框架](https://arxiv.org/abs/2312.05230) | NeurIPS 2023 Tutorial | 语言模型(L)、智能体模型(A)、世界模型(W) 三位一体 | [arXiv](https://arxiv.org/abs/2312.05230) |
| [从词模型到世界模型](https://arxiv.org/abs/2306.12672) | 2023.06 | LLM 翻译为概率程序，贝叶斯推理实现世界推理 | [arXiv](https://arxiv.org/abs/2306.12672) |
| [语言模型遇见世界模型](https://arxiv.org/abs/2305.10626) | 2023.05 | 具身经验微调 LM，物理推理提升 64.28% | [arXiv](https://arxiv.org/abs/2305.10626) |
| [LWM](https://arxiv.org/abs/2402.08268) | UC Berkeley, 2024 | 基于 RingAttention 的百万 token 视频+语言世界模型 | [GitHub](https://github.com/LargeWorldModel/LWM)（7.4k ⭐） |
| [LLM-Sim](https://arxiv.org/abs/2406.06485) | ACL 2024 | 语言模型作为文本世界模拟器 | [arXiv](https://arxiv.org/abs/2406.06485) |
| [WebDreamer](https://arxiv.org/abs/2411.06559) | 2024.11 | LLM 是否是互联网的世界模型 | [arXiv](https://arxiv.org/abs/2411.06559) |
| [WALL-E 2.0](https://arxiv.org/abs/2504.15785) | 2025.04 | 神经符号学习改善基于世界模型的 LLM Agent | [arXiv](https://arxiv.org/abs/2504.15785) |
| [WorldLLM](https://arxiv.org/abs/2506.06725) | 2025.06 | 好奇心驱动的理论构建改善 LLM 世界建模 | [arXiv](https://arxiv.org/abs/2506.06725) |
| [Affordances Enable Partial WM](https://arxiv.org/abs/2602.10390) | DeepMind, 2026.02 | 理论证明语言条件 Agent 必然拥有基于 affordance 的部分世界模型 | [arXiv](https://arxiv.org/abs/2602.10390) |
| [RL World Model for LLM Agents](https://arxiv.org/abs/2602.05842) | 2026.02 | 面向 LLM Agent 的强化世界模型学习 | [arXiv](https://arxiv.org/abs/2602.05842) |
| [On Emergent Social WMs](https://arxiv.org/abs/2602.10298) | 2026.02 | LLM 中涌现的社会世界模型——心智理论与语用推理 | [arXiv](https://arxiv.org/abs/2602.10298) |

### 8.4 方向小结

| 特征 | 描述 |
|------|------|
| **优势** | 利用 LLM 的广泛知识、支持自然语言交互、Agent+WM 范式兴起 |
| **局限** | LLM 的物理推理仍不可靠、幻觉问题、缺乏感知基础 |
| **核心洞察** | LLM 确实隐含学习了某些世界知识，但具身经验和结构化世界模型对于鲁棒的物理推理不可或缺 |
| **开放问题** | "LLM 是否真正具有世界模型"仍是开放性问题 |

---


# 方向六：JEPA 与表征空间预测

### 9.1 方向概述与 LeCun 的世界模型愿景

LeCun 2022 年的立场论文 [*A Path Towards Autonomous Machine Intelligence*](https://openreview.net/forum?id=BZ5a1r-kVsf) 提出了以世界模型为核心的自主智能体架构。

**JEPA 的核心主张**：
1. **在抽象表征空间中预测**，而非在像素空间中生成
2. **丢弃不可预测的信息**（噪声、无关细节），聚焦学习世界的**结构**
3. 生成式模型预测输出的每个细节——对高维输入（如视频）来说既浪费又不可行
4. 世界模型是架构的**核心组件**，支持预测、规划和推理

**为什么不用生成式模型？** 世界太复杂，无法在像素空间中预测——太多无关细节。JEPA 在学习到的表征空间中预测，聚焦于重要的内容。这与人类建模世界的方式一致：我们不预测每个光子——我们预测抽象状态和结果。

### 9.2 关键论文深度解析

#### 9.2.1 LeCun 立场论文（2022）— 理论蓝图

提出完整的自主智能体架构蓝图，包含世界模型、配置器（Configurator）、行动者（Actor）、感知模块、记忆模块和评价器（Critic），其中世界模型居于核心位置。

#### 9.2.2 V-JEPA 系列技术演进（2023–2025）

| 版本 | 时间 | 论文 | 核心贡献 |
|------|------|------|---------|
| **I-JEPA** | 2023.01 | [arXiv:2301.08243](https://arxiv.org/abs/2301.08243)（ICCV 2023） | 图像 JEPA：预测遮蔽 patch 的**表征**（非像素），ViT-Huge/14 在 ImageNet 上 <72h 训练 |
| **V-JEPA** | 2024.02 | [arXiv:2404.08471](https://arxiv.org/abs/2404.08471) | 视频 JEPA：遮蔽时空区域并在抽象空间预测，训练效率比生成式高 1.5-6 倍；首个在冻结评估中表现优异的视频模型 |
| **IWM** | 2024.03 | [arXiv:2403.00504](https://arxiv.org/abs/2403.00504) | 图像世界模型：将 JEPA 扩展到预测任意光度变换的效果 |
| **V-JEPA 2** | 2025.06 | [arXiv:2506.09985](https://arxiv.org/abs/2506.09985) | 统一理解、预测和规划的自监督视频模型 |
| **V-JEPA 2-AC** | 2025.06 | 同上 | Action-Conditioned 变体，扩展到具身控制 |

### 9.3 JEPA vs 生成式世界模型

| 维度 | JEPA（LeCun） | 生成式（Sora/Genie） |
|------|-------------|-------------------|
| **预测空间** | 抽象表征空间 | 像素/token 空间 |
| **不可预测细节** | 丢弃（聚焦结构） | 必须生成（浪费） |
| **训练效率** | 高 1.5-6 倍 | 需要海量计算 |
| **多任务复用** | 冻结编码器 + 轻量适配器 | 每个任务需完全微调 |
| **当前强项** | 感知、表征学习、具身控制 | 视觉丰富的交互式世界 |
| **规划集成** | 天然适合（在潜在空间中预测→规划） | 需要额外规划模块 |
| **成熟度** | 快速成熟中（V-JEPA 2 统一三任务） | 更成熟（Sora 2、Genie 3、Cosmos） |

**趋势**：两种范式正在融合——生成式模型日益学习结构化的潜在空间，JEPA 类模型也在扩展到更丰富的预测任务。最终可能需要**混合方法**——抽象世界模型用于规划 + 生成式解码器用于可视化和落地。

### 9.4 前沿论文全景

| 论文 | 时间 | 核心贡献 | 链接 |
|------|------|---------|------|
| [AD-L-JEPA](https://arxiv.org/abs/2501.04969) | 2025.01 | 自动驾驶 LiDAR 数据的 JEPA | [arXiv](https://arxiv.org/abs/2501.04969) |
| [seq-JEPA](https://arxiv.org/abs/2505.03176) | 2025.05 | 自回归预测不变-等变世界模型 | [arXiv](https://arxiv.org/abs/2505.03176) |
| [Causal-JEPA](https://arxiv.org/abs/2602.11389) | 2026.02 | 通过物体级潜在干预学习世界模型 | [arXiv](https://arxiv.org/abs/2602.11389) |
| [VLA-JEPA](https://arxiv.org/abs/2602.10098) | 2026.02 | 潜在世界模型增强 VLA 模型 | [arXiv](https://arxiv.org/abs/2602.10098) |
| Value-guided Action Planning with JEPA | ICLR 2026 Workshop | 基于 JEPA 的价值引导动作规划 | — |

### 9.5 方向小结

| 特征 | 描述 |
|------|------|
| **优势** | 训练效率高、抗噪声、编码结构化信息、天然适合规划 |
| **局限** | 不可直接可视化、生态成熟度低于生成式方法 |
| **趋势** | 在具身 AI 中持续发力（VLA-JEPA、V-JEPA 2-AC）；与生成式方法走向融合 |
| **代表方法** | I-JEPA、V-JEPA 系列、IWM、Causal-JEPA |

---


# 方向七：交互式游戏世界模型

### 10.1 方向概述

游戏环境是世界模型的天然试验场——明确的状态、动作、奖励，且视觉复杂度可控。2024 年起，这个方向出现了爆发式增长：**神经网络直接替代传统游戏引擎**，实时生成交互式游戏画面。

### 10.2 关键论文深度解析

#### 10.2.1 Genie 系列技术演进（2024–2025）

| 维度 | **Genie 1**（2024.02） | **Genie 2**（2024.12） | **Genie 3**（2025.08） |
|------|------|------|------|
| **参数** | 110 亿 | 未公开 | 未公开 |
| **架构** | 时空视频分词器 + 自回归动力学模型 + 潜在动作模型 | 自回归潜在扩散模型 | 自回归潜在扩散（升级） |
| **输入** | 无标注视频（无监督） | 单张图片 | 文本/图像 |
| **核心创新** | 从视频中**无监督发现潜在动作空间** | 涌现 NPC、物理、3D 记忆、最长 1 分钟一致 | **首个实时交互**（24fps/720p），文本可控天气/物体等事件 |
| **交互性** | 2D，有限控制 | 丰富 3D 世界，键盘/鼠标控制 | 实时 3D，高度可控 |
| **开放性** | 闭源 | 闭源（博客） | 闭源（Project Genie 试用） |
| **链接** | [arXiv](https://arxiv.org/abs/2402.15391) | [Blog](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/) | [Blog](https://deepmind.google/blog/genie-3-a-new-frontier-for-world-models/) |

**Genie 2 涌现能力**：正确识别并移动玩家角色（非背景）；反事实生成（同一起始帧不同动作产生不同轨迹）；**长期记忆**（记住被遮挡的世界部分并在可见时准确渲染）；NPC 行为、物理模拟（水、烟、重力、光照、反射）。与 **SIMA 2 智能体**集成后，可在生成的环境中遵循自然语言指令。

#### 10.2.2 GameNGen — 扩散模型作为实时游戏引擎

| 项目 | 内容 |
|------|------|
| **论文** | [*Diffusion Models Are Real-Time Game Engines*](https://arxiv.org/abs/2408.14837) |
| **发表** | **ICLR 2025** |

**方法**：两阶段训练——（1）RL 智能体玩 DOOM，记录 (动作, 帧) 对；（2）扩散模型学习给定过去帧和动作预测下一帧。

**成果**：单 TPU 上 **20 fps 实时运行** DOOM。下一帧预测 PSNR 达 29.4（接近有损 JPEG 压缩质量）。人类评估者在区分真实游戏和模拟时**仅略好于随机**。

### 10.3 前沿论文全景

| 论文 | 时间 | 核心贡献 | 链接 |
|------|------|---------|------|
| [Oasis](https://oasis-model.github.io/) | Decart, 2024.10 | 纯 Transformer 实时开放世界模拟，20fps，500M 开源 | [GitHub](https://github.com/etched-ai/open-oasis)（2.1k ⭐） |
| [GameGen-X](https://arxiv.org/abs/2411.00769) | 2024.11 | 首个扩散 Transformer 游戏视频生成模型，百万级数据集 | [arXiv](https://arxiv.org/abs/2411.00769) |
| [GameFactory](https://arxiv.org/abs/2501.08325) | ICCV 2025 Highlight | 跨场景泛化的动作可控游戏视频生成 | [arXiv](https://arxiv.org/abs/2501.08325) |
| [Pandora](https://arxiv.org/abs/2406.09455) | 2024.06 | 自然语言控制的通用世界模型，混合自回归-扩散 | [arXiv](https://arxiv.org/abs/2406.09455) |
| [iVideoGPT](https://arxiv.org/abs/2405.15223) | NeurIPS 2024 | 交互式 VideoGPT，统一视觉/动作/奖励 token | [arXiv](https://arxiv.org/abs/2405.15223) |
| [Hunyuan-GameCraft-2](https://arxiv.org/abs/2511.23429) | 腾讯, 2025.11 | 指令驱动的交互式游戏世界模型 | [arXiv](https://arxiv.org/abs/2511.23429) |
| [MineWorld](https://arxiv.org/abs/2504.08388) | 2025.04 | Minecraft 上的实时开源交互世界模型 | [arXiv](https://arxiv.org/abs/2504.08388) |
| [LingBot-World](https://arxiv.org/abs/2601.20540) | 2026.01 | 开源世界模拟器，分钟级长时记忆 + 实时交互 | [arXiv](https://arxiv.org/abs/2601.20540) |
| [Infinite-World](https://arxiv.org/abs/2602.02393) | 2026.02 | 交互式世界模型扩展至 1000 帧长时域 | [arXiv](https://arxiv.org/abs/2602.02393) |
| [Yume / Yume-1.5](https://arxiv.org/abs/2507.17744) | 2025 | 文本控制的交互式世界生成 | [arXiv](https://arxiv.org/abs/2507.17744) |

### 10.4 方向小结

| 特征 | 描述 |
|------|------|
| **优势** | 可直接评估交互质量、用户体验直观、基准明确 |
| **局限** | 多数仍限于特定游戏/场景，泛化能力有限 |
| **趋势** | 从单游戏引擎到跨域泛化（GameFactory）；从离散 token 到连续扩散；实时化 |
| **代表方法** | Genie 系列、GameNGen、DIAMOND、Oasis、GameFactory |

---


# 方向八：3D/4D 几何感知世界模型

### 11.1 方向概述

2025 年涌现的新方向：超越 2D 视频的世界模型，构建几何感知的 3D/4D 世界理解和生成。这对于机器人和具身 AI 至关重要——真实世界本质上是三维的。3D 高斯溅射（3DGS）正在成为主要的表示形式。

### 11.2 关键论文深度解析

#### 11.2.1 Aether — 几何感知的统一世界建模

| 项目 | 内容 |
|------|------|
| **论文** | [*Aether: Geometric-Aware Unified World Modeling*](https://arxiv.org/abs/2503.18945) |
| **机构** | InternRobotics |
| **荣誉** | **ICCV 2025 杰出论文奖 & RIWM Outstanding Paper** |
| **代码** | [github.com/InternRobotics/Aether](https://github.com/InternRobotics/Aether)（573 ⭐） |

Aether 将几何感知引入世界模型，实现跨任务的统一建模（深度估计、光流、相机姿态 + 视频预测）。其获得 ICCV 2025 杰出论文奖表明学术界对 3D/4D 世界模型方向的高度认可。

### 11.3 前沿论文全景

| 论文 | 机构/时间 | 核心贡献 | 链接 |
|------|----------|---------|------|
| [FlexWorld](https://arxiv.org/abs/2503.13265) | 清华/人大, 2025.03 | 渐进式 3D 场景扩展，V2V 扩散 + 几何融合 | [arXiv](https://arxiv.org/abs/2503.13265) |
| [**HunyuanWorld 1.0**](https://arxiv.org/abs/2507.21809) | 腾讯, 2025.07 | 从文字/图像生成沉浸式可探索交互 3D 世界 | [GitHub](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0)（2.7k ⭐） |
| HunyuanWorld-Voyager | 腾讯 | 相机控制的交互 RGBD 视频 + 实时 3D 重建 | [GitHub](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0) |
| HunyuanWorld-Mirror | 腾讯 | 快速通用 3D 重建模型 | — |
| [HY-WorldPlay](https://arxiv.org/abs/2512.14614) | 腾讯, 2025.12 | 实时延迟 + 几何一致的交互式世界建模框架 | [arXiv](https://arxiv.org/abs/2512.14614) |
| [Geometry Forcing](https://arxiv.org/abs/2507.07982) | 2025.07 | 视频扩散与 3D 表示融合实现一致世界建模 | [arXiv](https://arxiv.org/abs/2507.07982) |
| [Terra](https://arxiv.org/abs/2510.14977) | 2025.10 | 基于点潜在的可探索原生 3D 世界模型 | [arXiv](https://arxiv.org/abs/2510.14977) |
| [EvoWorld](https://arxiv.org/abs/2510.01183) | 2025.10 | 带显式 3D 记忆的全景世界生成 | [arXiv](https://arxiv.org/abs/2510.01183) |
| [WorldGen](https://arxiv.org/abs/2511.16825) | Meta, 2025.11 | 从文本生成可穿越可交互 3D 世界 | [arXiv](https://arxiv.org/abs/2511.16825) |
| [**OmniWorld**](https://arxiv.org/abs/2509.12201) | ICLR 2026 | 多域多模态 4D 世界建模数据集 | [arXiv](https://arxiv.org/abs/2509.12201) |
| [NeoVerse](https://arxiv.org/abs/2601.00393) | 2026.01 | 基于真实单目视频增强 4D 世界模型 | [arXiv](https://arxiv.org/abs/2601.00393) |
| [TeleWorld](https://arxiv.org/abs/2601.00051) | 2026.01 | 动态多模态合成 4D 世界模型 | [arXiv](https://arxiv.org/abs/2601.00051) |
| [WonderZoom](https://arxiv.org/abs/2512.09164) | 2025.12 | 多尺度 3D 世界生成 | [arXiv](https://arxiv.org/abs/2512.09164) |

**腾讯混元世界系列**形成了该方向最完整的产品矩阵：HunyuanWorld-1.0（3D 生成）→ Voyager（交互式 RGBD）→ Mirror（快速 3D 重建）→ WorldPlay（实时几何一致性），代表了从视频生成向真正 3D 世界模型最显式的推进。

### 11.4 方向小结

| 特征 | 描述 |
|------|------|
| **优势** | 几何精确、物理一致、连接视频世界模型与真实物理世界部署 |
| **局限** | 数据获取难、实时渲染成本高、泛化受限 |
| **趋势** | 3D 高斯溅射成为主要表示；4D（3D + 时间）成为新热点；向可交互、可探索方向发展 |
| **代表方法** | Aether、HunyuanWorld 系列、FlexWorld、OmniWorld |

---


# 方向九：领域特定世界模型

### 12.1 方向概述

世界模型正在快速渗透到各个垂直领域，从医疗健康到科学模拟、从外科手术到网络通信。这标志着世界模型从通用研究工具走向行业应用的转折。

### 12.2 医疗健康

| 论文 | 时间 | 核心贡献 | 链接 |
|------|------|---------|------|
| [CheXWorld](https://arxiv.org/abs/2504.13820) | CVPR 2025 | 胸片世界模型用于表示学习 | [arXiv](https://arxiv.org/abs/2504.13820) |
| [EchoWorld](https://arxiv.org/abs/2504.13065) | CVPR 2025 | 超声心动图探针引导世界模型 | [arXiv](https://arxiv.org/abs/2504.13065) |
| [Medical World Model](https://arxiv.org/abs/2506.02327) | 2025.06 | 肿瘤演进的生成模拟 | [arXiv](https://arxiv.org/abs/2506.02327) |
| [CLARITY](https://arxiv.org/abs/2512.08029) | 2025.12 | 潜在空间中的疾病轨迹建模 | [arXiv](https://arxiv.org/abs/2512.08029) |
| [EHRWorld](https://arxiv.org/abs/2602.03569) | 2026.02 | 以患者为中心的长程临床轨迹世界模型 | [arXiv](https://arxiv.org/abs/2602.03569) |
| [MRI Contrast WM](https://arxiv.org/abs/2602.19503) | 2026.02 | 世界模型用于 MRI 造影增强动力学建模 | [arXiv](https://arxiv.org/abs/2602.19503) |

### 12.3 科学模拟

| 论文 | 时间 | 核心贡献 | 链接 |
|------|------|---------|------|
| [PhysFire-WM](https://arxiv.org/abs/2512.17152) | 2025.12 | 物理驱动的火灾蔓延世界模型 | [arXiv](https://arxiv.org/abs/2512.17152) |
| ODesign | 2025.10 | 生物分子交互设计的世界模型 | — |
| [VCWorld](https://arxiv.org/abs/2512.00306) | 2025.12 | 虚拟细胞模拟的生物世界模型 | [arXiv](https://arxiv.org/abs/2512.00306) |
| Remote Sensing WM | 2025.09 | 遥感世界模型 | — |
| [Transient Heat Transfer WM](https://arxiv.org/abs/2601.22086) | 2026.01 | 几何感知世界模型学习瞬态对流换热（PDE 代理） | [arXiv](https://arxiv.org/abs/2601.22086) |

### 12.4 外科手术

| 论文 | 时间 | 核心贡献 | 链接 |
|------|------|---------|------|
| [SurgWorld](https://arxiv.org/abs/2512.23162) | 2025.12 | 从视频学习手术机器人策略 | [arXiv](https://arxiv.org/abs/2512.23162) |
| [Cosmos-Surg-dVRK](https://arxiv.org/abs/2510.16240) | 2025.10 | 手术机器人策略学习的自动评估 | [arXiv](https://arxiv.org/abs/2510.16240) |

### 12.5 网络与通信

| 论文 | 时间 | 核心贡献 | 链接 |
|------|------|---------|------|
| MobiWorld | 2025.07 | 移动无线网络世界模型 | — |
| [NetWorld](https://arxiv.org/abs/2602.00558) | 2026.02 | 扩散世界模型用于无线网络多智能体 RL | [arXiv](https://arxiv.org/abs/2602.00558) |

### 12.6 其他领域

| 论文 | 领域 | 时间 | 核心贡献 | 链接 |
|------|------|------|---------|------|
| [Active Inference UAV Swarm](https://arxiv.org/abs/2601.14354) | 无人机 | 2026.01 | 主动推理驱动的世界建模用于 UAV 蜂群 | [arXiv](https://arxiv.org/abs/2601.14354) |
| [Semantic Belief-State WM](https://arxiv.org/abs/2601.03517) | 人体运动 | 2026.01 | 语义信念状态世界模型用于 3D 人体运动预测 | [arXiv](https://arxiv.org/abs/2601.03517) |
| [Executable Ontologies](https://arxiv.org/abs/2601.09452) | 游戏开发 | 2026.01 | 可执行本体论作为语义世界建模范式 | [arXiv](https://arxiv.org/abs/2601.09452) |

### 12.7 方向小结

领域特定世界模型的渗透速度表明，世界模型正在从一种研究方法论演变为通用的计算工具。医疗健康领域尤其活跃（6 篇论文覆盖胸片、超声心动图、肿瘤、电子病历、MRI），预示着世界模型在高风险决策场景的巨大潜力。

---


# 全球企业与机构布局

### 13.1 科技巨头战略图谱

| 机构 | 代表成果 | 战略定位 | 关键特征 |
|------|---------|---------|---------|
| **NVIDIA** | [Cosmos](https://arxiv.org/abs/2501.03575)、Cosmos-Transfer1、Cosmos-Reason1 | 物理世界基础模型 + 数据飞轮 | 全栈布局：预训练→微调→部署；开源 Cosmos 系列；DGX 云服务闭环 |
| **Meta (FAIR)** | [V-JEPA 2](https://arxiv.org/abs/2412.04468)、V-JEPA 2-AC、JEPA 路线 | 非生成式世界理解 | LeCun 主导 JEPA 路线；不追求像素生成，聚焦语义预测；具身 AI 闭环 |
| **Google DeepMind** | [Genie 2](https://arxiv.org/abs/2501.18517)、[SIMA 2](https://arxiv.org/abs/2502.16399)、Dreamer V4 | 交互式环境模拟 | Genie 系列创造交互世界；SIMA 系列实现跨游戏通用智能体；强化学习基因 |
| **OpenAI** | Sora / Sora 2 | 视频生成定义者 | 首次将视频生成定义为"世界模拟器"；引爆行业关注；策略转向产品 |
| **Microsoft** | [WorldDreamer](https://arxiv.org/abs/2401.09985)、DIAMOND | 多模态 + 游戏 | 收购 Activision 后在游戏世界模型发力；DIAMOND 证明扩散模型可替代传统 RL 环境 |
| **腾讯** | [HunyuanVideo](https://arxiv.org/abs/2412.03603)、[HunyuanWorld](https://arxiv.org/abs/2501.12513) | 视频+3D 双线 | HunyuanVideo 开源高质量基座；HunyuanWorld 探索 3D 驾驶模拟；微信生态应用 |
| **字节跳动** | [Seedance](https://arxiv.org/abs/2503.07703)、SkyReels-V2、[AniSora](https://arxiv.org/abs/2412.10255) | 视频生成工厂 | Seedance 高一致性动画；SkyReels 无限时长；AniSora 动漫可控；抖音生态 |
| **阿里巴巴** | [Wan2.1](https://arxiv.org/abs/2503.20314)、[CogVideoX](https://arxiv.org/abs/2408.06072) | 开源视频生态 | Wan2.1 全面开源（1.3B–14B）；CogVideoX 全链路开源；通义系列 |
| **快手** | [Step-Video](https://arxiv.org/abs/2502.10248)、[Kling](https://klingai.kuaishou.com) | 长视频+物理一致 | Step-Video 步长自适应；Kling 实时物理模拟；快手 APP 内嵌 |
| **Wayve** | [GAIA-1/2/3](https://arxiv.org/abs/2309.17080) | 端到端自动驾驶 | GAIA 系列从规划到世界模型的全栈方案；伦敦路测；融资 $10.6 亿 |
| **1X Technologies** | [World Model for Humanoid Robot](https://arxiv.org/abs/2501.10100) | 人形机器人 | 世界模型驱动的人形机器人运动规划；实体部署；挪威独角兽 |
| **Runway** | Gen-3 Alpha | 创意视频工具 | 视频生成创业标杆；从工具走向平台；融资 $15 亿 |

### 13.2 核心学术机构

| 机构 | 代表方向 | 关键成果 |
|------|---------|---------|
| **UC Berkeley** | RL + 视频 | UniSim、Dreamer 系列合作、DIAMOND |
| **MIT** | 认知科学 + RL | MuZero 理论、认知世界模型 |
| **NYU (Yann LeCun 组)** | JEPA | I-JEPA、V-JEPA、IWM，JEPA 路线图的核心产出地 |
| **CMU** | 具身 AI | 多个 VLA+WM 融合工作 |
| **清华大学** | 多方向 | CogVideoX、AniSora、多个具身 AI 工作 |
| **北京大学** | 视频 + 3D | Aether、World3D 相关 |
| **上海 AI Lab** | 视频+驾驶 | DriveDreamer 系列、Vista |
| **Google Research / DeepMind** | 全方向 | Genie 系列、Dreamer 系列、SIMA 系列 |

### 13.3 视频基座模型架构趋同分析

2024–2025 年间，各机构的视频生成世界模型在架构选择上出现高度趋同，形成 **DiT + 3D VAE + Flow Matching** 的主流技术栈：

| 技术栈组件 | 选择 | 采用者 | 替代方案 |
|-----------|------|--------|---------|
| **骨干网络** | DiT (Diffusion Transformer) | Sora、HunyuanVideo、Wan2.1、Step-Video、SkyReels-V2、Seedance | U-Net (早期方案，已淡出) |
| **视频编码器** | 3D VAE / Causal 3D VAE | HunyuanVideo、CogVideoX、Wan2.1、COSMOS | 2D VAE + 时间层 (效率低) |
| **采样策略** | Flow Matching (Rectified Flow) | Wan2.1、SkyReels-V2、HunyuanVideo | DDPM/DDIM (步数多，效率低) |
| **文本编码器** | 双编码器 (CLIP + T5/LLM) | CogVideoX、Step-Video、Wan2.1 | 单 CLIP (语义不足) |
| **长度扩展** | 分块推理 / 滑窗 / 无限AR | SkyReels-V2、Kling、Wan2.1 | 全序列注意力 (显存爆炸) |

**趋同驱动力**：

1. **DiT 的可扩展性**：Transformer 架构天然适配大规模并行训练，scaling law 明确
2. **3D VAE 的时空压缩效率**：将视频从像素空间压缩到紧凑的潜在空间，减少 $16 \times$ 以上计算量
3. **Flow Matching 的采样效率**：直线传输路径使 4–8 步采样即可达到高质量，远优于 DDPM 的 50–1000 步
4. **训练稳定性**：三者组合在大规模训练中表现出更好的梯度行为和收敛特性

### 13.4 全球竞争格局分析

**三大阵营**：

1. **北美 — 技术定义者**：OpenAI（产品化路线）、NVIDIA（全栈基座）、Meta（非生成路线）、Google（交互智能体）
2. **中国 — 开源追赶者**：腾讯/阿里/字节/快手在视频生成领域快速跟进并大规模开源，形成独特的"大厂+开源"模式。上海 AI Lab、清华等在自动驾驶和具身 AI 方向领先
3. **欧洲 — 垂直深耕者**：Wayve（自动驾驶）、1X（人形机器人）、NNAISENSE（工业 RL）在细分场景深度积累

**竞争趋势**：
- 视频生成方向趋向同质化，差异化转向应用场景和数据飞轮
- 具身 AI 方向成为新高地，Meta 和 Google 投入最大
- 自动驾驶方向由 Wayve、NVIDIA、上海 AI Lab 三足鼎立
- 开源力度加大：中国大厂和 NVIDIA 主导开源浪潮

---


# 开源生态全景

### 14.1 世界模型核心项目

| 项目 | 机构 | Stars | 许可证 | 核心特征 | 链接 |
|------|------|-------|--------|---------|------|
| **Cosmos** | NVIDIA | 5k+ | Apache 2.0 | 物理世界基础模型，预训练+微调全链路 | [GitHub](https://github.com/NVIDIA/Cosmos) |
| **CogVideoX** | 智谱 AI / 清华 | 10k+ | Apache 2.0 | 全链路开源视频生成（训练+推理+数据） | [GitHub](https://github.com/THUDM/CogVideo) |
| **Wan2.1** | 阿里巴巴 | 15k+ | Apache 2.0 | 1.3B–14B 多规模视频生成 | [GitHub](https://github.com/Wan-Video/Wan2.1) |
| **HunyuanVideo** | 腾讯 | 8k+ | Tencent | 高质量长视频生成 | [GitHub](https://github.com/Tencent/HunyuanVideo) |
| **SkyReels-V2** | 昆仑万维 | 3k+ | Apache 2.0 | 无限时长视频+物理世界模型 | [GitHub](https://github.com/SkyworkAI/SkyReels-V2) |
| **DreamerV3** | Danijar Hafner | 2k+ | MIT | 通用强化学习世界模型 | [GitHub](https://github.com/danijar/dreamerv3) |
| **DIAMOND** | Microsoft | 1k+ | MIT | 扩散世界模型玩 Atari | [GitHub](https://github.com/eloialonso/diamond) |
| **AniSora** | 清华 | 1k+ | Apache 2.0 | 动漫可控视频生成 | [GitHub](https://github.com/bilibili/AniSora) |
| **LWM** | UC Berkeley | 800+ | Apache 2.0 | 大世界模型（百万 token 上下文） | [GitHub](https://github.com/LargeWorldModel/LWM) |

### 14.2 上游基础设施

| 项目 | 功能 | 链接 |
|------|------|------|
| **Diffusers** (HuggingFace) | 扩散模型标准库，支持 DiT/Flow Matching | [GitHub](https://github.com/huggingface/diffusers) |
| **xDiT** | DiT 模型分布式推理加速 | [GitHub](https://github.com/xdit-project/xDiT) |
| **Open-Sora** (HPC-AI Tech) | Sora 复现框架 | [GitHub](https://github.com/hpcaitech/Open-Sora) |
| **Open-Sora-Plan** (PKU) | Sora 复现计划 | [GitHub](https://github.com/PKU-YuanGroup/Open-Sora-Plan) |
| **Gymnasium** (Farama) | RL 环境标准接口 | [GitHub](https://github.com/Farama-Foundation/Gymnasium) |
| **MineRL** | Minecraft RL 环境 | [GitHub](https://github.com/minerllabs/minerl) |

### 14.3 开源成熟度评估

| 维度 | 🟢 成熟 | 🟡 发展中 | 🔴 早期 |
|------|---------|----------|---------|
| **视频生成** | CogVideoX、Wan2.1（全链路） | HunyuanVideo（推理为主） | Sora（未开源） |
| **RL 世界模型** | DreamerV3、DIAMOND | TD-MPC2 | Dreamer V4（未开源） |
| **自动驾驶** | — | Vista（部分开源） | GAIA 系列（未开源） |
| **具身 AI** | — | — | 多数为论文复现 |
| **物理基座** | Cosmos（全栈开源） | — | — |
| **JEPA** | — | V-JEPA（权重开放） | V-JEPA 2（未开源） |

**开源趋势**：
- 中国机构（阿里、腾讯、智谱、昆仑万维）在视频生成方向的开源力度全球领先
- NVIDIA Cosmos 定义了"预训练基座 + 领域微调"的开源范式
- RL 世界模型开源生态最为成熟（DreamerV3 可直接复现）
- 具身 AI 和自动驾驶方向开源严重不足，制约社区发展

---


# 技术路线对比分析

### 15.1 两大范式：生成式 vs 非生成式

| 维度 | 生成式世界模型 | 非生成式世界模型 (JEPA) |
|------|--------------|----------------------|
| **代表** | Sora, Cosmos, Dreamer, GAIA | V-JEPA, I-JEPA, IWM |
| **预测目标** | 像素/token 级重建 | 潜在表征级预测 |
| **损失函数** | 重建损失 / 扩散损失 / AR 交叉熵 | 对比 + 正则化 (VICReg 等) |
| **解码需求** | 需要解码器还原像素 | 无需像素解码 |
| **计算效率** | 高（需生成高维输出） | 低（仅在潜在空间运算） |
| **物理一致性** | 可学习但不保证 | 可通过结构化潜在空间嵌入 |
| **动作可控性** | 难（需额外条件注入） | V-JEPA 2-AC 原生支持 |
| **可视化能力** | 强（可生成视频/图像） | 弱（潜在空间不可直接可视化） |
| **应用场景** | 视频生成、驾驶模拟、游戏 | 具身 AI、视频理解、规划 |

**核心分歧**：LeCun 认为像素级生成是浪费计算资源，因为高频细节对决策无关紧要。但生成式路线的支持者认为，像素级生成是验证物理理解的最佳方式——"如果你能生成它，你就理解它"。

### 15.2 架构演进路线

```
[2018] VAE + MDN-RNN (World Models)
  ↓
[2020] Transformer AutoRegressive (MuZero 系列)
  ↓
[2022] Diffusion + U-Net (早期扩散世界模型)
  ↓
[2024] DiT + 3D VAE + Flow Matching (当前主流)
  ↓
[2025] 多模态融合：DiT + Action Tokens + Physics Priors
  ↓
[2026?] 统一世界模型：生成 + 理解 + 规划一体化
```

### 15.3 核心维度对比

| 维度 | RL 世界模型 | 视频生成 | 自动驾驶 | 具身 AI | LLM | JEPA | 游戏 | 3D/4D |
|------|-----------|---------|---------|--------|-----|------|------|-------|
| 动作条件 | ✅ 原生 | ❌→🟡 | ✅ 驾驶动作 | ✅ 关键 | 🟡 文本 | ✅ V-JEPA 2-AC | ✅ 游戏动作 | 🟡 相机轨迹 |
| 长程一致 | 🟡 有限 | 🟡 改善中 | ✅ 关键需求 | 🟡 | ✅ LLM 优势 | ✅ 时间抽象 | 🟡 | 🟡 |
| 物理准确 | 🟡 | 🟡 | ✅ 安全关键 | ✅ 操作需求 | ❌ | 🟡 | 🟡 游戏物理 | ✅ 3D 几何 |
| 实时推理 | ✅ 低维 | ❌ 慢 | ✅ 必须 | ✅ 必须 | 🟡 | ✅ 高效 | ✅ 20fps+ | ❌ 慢 |
| 泛化能力 | 🟡 任务受限 | ✅ 通用 | 🟡 场景受限 | 🟡 | ✅ 最强 | ✅ 零样本 | 🟡 游戏受限 | 🟡 |

### 15.4 动作可控性问题

动作可控性是世界模型从"观察者"走向"参与者"的关键。各方向的解决方案：

1. **显式动作 token**：Genie 系列通过学习潜在动作空间（LAM），从纯视频中提取动作表征
2. **条件注入**：GAIA / Vista 等将驾驶动作作为条件信号注入扩散过程
3. **VLA 融合**：具身 AI 方向通过 VLA 架构实现从视觉观测到动作输出的端到端映射
4. **JEPA 原生支持**：V-JEPA 2-AC 在潜在空间中直接预测动作后果，无需像素解码
5. **交互式训练**：SIMA 2 通过大规模交互数据训练，实现自然语言指令到游戏动作的映射

### 15.5 缩放定律的跨域验证

视频生成方向已初步验证 scaling law：

- **Wan2.1**：1.3B → 14B 参数，视觉质量和物理一致性持续提升
- **Step-Video**：30B 参数，证明更大模型带来更好的时间一致性
- **Cosmos**：从 4B 到 14B，预训练数据量从 2000 万到 1 亿视频

但 **scaling law 的极限** 尚未明确：
- 更大的模型是否能涌现"真正的物理理解"？
- 数据质量 vs 数据数量的权衡点在哪里？
- 计算效率如何随规模变化？

---


# 评估基准景观

世界模型评估正在从"单维度指标"走向"多维度综合基准"。以下是截至 2026 年 2 月的主要评估基准：

| 基准 | 时间 | 评估目标 | 核心维度 | 链接 |
|------|------|---------|---------|------|
| [**WorldSimBench**](https://arxiv.org/abs/2410.18072) | 2024.10 | 视频生成世界模拟器 | 显式感知 + 隐式推理 | [arXiv](https://arxiv.org/abs/2410.18072) |
| [**EWMBench**](https://arxiv.org/abs/2405.13503) | 2024.05 | 具身世界模型 | 物理交互预测 | [arXiv](https://arxiv.org/abs/2405.13503) |
| [**WorldModelBench**](https://arxiv.org/abs/2504.12579) | 2025.04 | 多模态世界模型 | 感知、推理、规划统一评估 | [arXiv](https://arxiv.org/abs/2504.12579) |
| [**WorldArena**](https://arxiv.org/abs/2501.18622) | 2025.01 | 交互式智能体评估 | 智能体在世界模型中的表现 | [arXiv](https://arxiv.org/abs/2501.18622) |
| [**WorldBench**](https://arxiv.org/abs/2504.18308) | 2025.04 | LLM 世界知识 | 常识推理、空间推理 | [arXiv](https://arxiv.org/abs/2504.18308) |
| [**4DWorldBench**](https://arxiv.org/abs/2601.15655) | 2026.01 | 4D 时空理解 | 3D + 时间维度预测 | [arXiv](https://arxiv.org/abs/2601.15655) |
| [**PhysBench**](https://arxiv.org/abs/2406.10170) | 2024.06 | 物理推理 | 刚体/流体/材料等物理规律 | [arXiv](https://arxiv.org/abs/2406.10170) |
| [**PhysGame**](https://arxiv.org/abs/2412.01800) | 2024.12 | 游戏物理理解 | 视频生成模型的游戏物理表现 | [arXiv](https://arxiv.org/abs/2412.01800) |
| [**VGBench**](https://arxiv.org/abs/2309.16292) | 2023.09 | 视频生成质量 | FVD、IS、CLIP-Sim 等 | [arXiv](https://arxiv.org/abs/2309.16292) |
| [**RoboBench (Dojo)**](https://arxiv.org/abs/2501.05600) | 2025.01 | 机器人世界模型 | 操作任务成功率 | [arXiv](https://arxiv.org/abs/2501.05600) |
| [**DriveArena**](https://arxiv.org/abs/2408.00415) | 2024.08 | 自动驾驶闭环 | 规划安全性、合规性 | [arXiv](https://arxiv.org/abs/2408.00415) |

**评估维度演进**：

1. **第一代（~2023）**：FVD、FID 等视觉质量指标 → 只关注"生成的像不像"
2. **第二代（2024）**：物理一致性 + 因果推理 → 关注"理解了没有"（PhysBench、WorldSimBench）
3. **第三代（2025–2026）**：交互性 + 动作可控 + 闭环评估 → 关注"能不能用"（WorldArena、4DWorldBench）

---


# 关键趋势与未来展望

### 17.1 研究爆发的量化观察

| 时间段 | 估计论文数量 | 代表事件 |
|--------|------------|---------|
| 2018–2022 | ~20 篇/年 | World Models、Dreamer V1–V2、MuZero |
| 2023 | ~40 篇 | GAIA-1、DreamerV3、LLM as WM 兴起 |
| 2024 | ~80 篇 | Sora 发布引爆行业、视频生成军备竞赛、JEPA 路线确立 |
| 2025.01–2026.02 | ~100+ 篇 | 具身 AI 融合、3D/4D 世界模型、领域渗透、中国开源浪潮 |

从 2024 年 Sora 发布起，世界模型论文呈指数级增长。2025 年仅前两个月即产出超过 50 篇相关论文。

### 17.2 九大趋势

**趋势一：从像素生成到物理理解**
视频生成模型正在从"生成逼真画面"走向"理解物理规律"。COSMOS-Reason1 等工作开始将物理推理能力嵌入世界模型。

**趋势二：从被动观察到主动交互**
Genie 2、SIMA 2、V-JEPA 2-AC 代表了世界模型从被动的视频预测走向主动的交互式模拟。

**趋势三：生成式与非生成式路线融合**
两大范式不再对立。IWM 等工作开始探索"在潜在空间做规划，在像素空间做验证"的混合路线。

**趋势四：DiT + 3D VAE + Flow Matching 成为视频基座标配**
各机构的视频生成模型在架构选择上高度趋同（详见 13.3 节）。

**趋势五：VLA + 世界模型深度融合**
具身 AI 方向出现五种 VLA+WM 融合范式（详见 7.3 节），世界模型正在成为机器人策略学习的核心组件。

**趋势六：3D/4D 几何感知成为新前沿**
从 2D 视频到 3D 世界的跨越正在发生。Aether、HunyuanWorld、World3D 等工作将 3D 几何理解嵌入世界模型。

**趋势七：领域特定世界模型快速扩散**
医疗（6 篇）、手术（2 篇）、科学模拟（5 篇）、网络（2 篇）等垂直领域快速涌现专用世界模型。

**趋势八：中国开源力量崛起**
阿里 Wan2.1、腾讯 HunyuanVideo、智谱 CogVideoX、昆仑 SkyReels-V2 等构成了全球最活跃的视频生成开源生态。

**趋势九：评估基准体系化**
从 FVD/FID 单一指标走向 WorldSimBench、4DWorldBench 等多维综合基准，评估从"像不像"升级到"理解了没有"和"能不能用"。

### 17.3 当前 SOTA 成绩汇总

| 任务 | SOTA 方法 | 关键指标 | 时间 |
|------|----------|---------|------|
| Atari 100K 效率 | Dreamer V4 | 超人类水平 26/26 | 2025.01 |
| 视频生成质量 | Wan2.1-14B | VBench 综合第一 | 2025.03 |
| 驾驶场景生成 | Vista | 长程一致性最佳 | 2024.01 |
| 具身 AI 预训练 | V-JEPA 2-AC | ImageNet 零样本 SOTA | 2025.02 |
| 交互式世界生成 | Genie 2 | 无限时长 3D 交互 | 2025.01 |
| 3D 世界生成 | Aether | 4 任务统一 SOTA | 2025.02 |
| 物理基座模型 | Cosmos | 9B 参数最大世界基座 | 2025.01 |

### 17.4 主要挑战

1. **物理一致性不足**：当前视频生成模型仍会违反基本物理规律（物体穿模、影子消失、重力异常）
2. **长程一致性衰减**：生成时长超过 30 秒后，场景一致性和物体身份保持急剧下降
3. **因果推理能力缺失**：大多数模型是相关性建模而非因果建模，无法进行反事实推理
4. **评估标准碎片化**：缺乏统一的、被广泛认可的世界模型评估框架
5. **计算成本高昂**：视频生成世界模型训练成本极高（Wan2.1-14B 需数千 GPU 天）
6. **Sim-to-Real 鸿沟**：模拟环境中训练的策略在真实世界部署时仍面临巨大性能下降
7. **安全与可靠性**：自动驾驶等安全关键场景要求极高的可靠性保证
8. **数据质量瓶颈**：高质量、多样化的训练数据仍是稀缺资源

### 17.5 未来展望

**短期（2026–2027）**：
- 视频生成模型将实现分钟级高质量生成
- 具身 AI 世界模型将在实验室环境中闭环运行
- 自动驾驶世界模型将支撑 L3+ 场景

**中期（2027–2030）**：
- 统一世界模型：生成 + 理解 + 规划一体化
- 通用物理引擎：从视频学习到的物理规律可迁移到新场景
- 交互式世界模拟器成为 AI 系统的标准组件

**长期（2030+）**：
- 接近 Kenneth Craik（1943）的愿景："心智携带世界的小规模模型"
- 世界模型作为 AGI 的核心认知模块
- 人类与 AI 在共享世界模型中协作

---


# 结论

世界模型是 AI 系统理解和预测物理世界的核心能力。本报告系统梳理了从 1943 年 Kenneth Craik 提出"心智模型"假说到 2026 年 2 月的完整发展脉络，覆盖 9 个研究方向、约 210 篇代表性论文、12 家全球机构和 11 个评估基准。

**关键发现**：

1. **2024 年是世界模型的奇点**：Sora 的发布将世界模型从学术概念推向产业焦点，触发了视频生成领域的全球军备竞赛
2. **技术路线正在收敛**：DiT + 3D VAE + Flow Matching 成为视频基座标配；生成式与非生成式路线从对立走向互补
3. **应用场景快速扩展**：从 Atari 游戏到自动驾驶、从手术机器人到气候模拟，世界模型正渗透到每个需要物理理解的领域
4. **中国力量不可忽视**：在视频生成开源生态、自动驾驶模拟、具身 AI 等方向，中国机构（阿里、腾讯、字节、上海 AI Lab、清华）贡献了大量高质量工作
5. **最大瓶颈仍是物理理解**：生成逼真画面不等于理解物理规律，bridging this gap 是下一阶段的核心挑战

世界模型正处于从"模式识别"到"世界理解"的关键转折点。我们有理由相信，未来 5 年内世界模型将成为 AI 系统的标准认知组件，如同今天的 Transformer 之于语言理解。

---


# 论文与参考文献总索引

> 本附录统一收录本报告涉及的所有论文，按类型分为**研究论文**和**综述论文**两部分。所有论文在正文中均以行内超链接形式引用。方向编号（D1–D9）对应本报告 9 个研究方向。

### 一、研究论文索引

> 按发表时间排序，共收录约 200 篇代表性论文。完整论文列表请参见各方向全景表（Section 四–十二）。

| # | 论文 | 时间 | 方向 | 机构 | 链接 |
|---|------|------|------|------|------|
| 1 | World Models (Ha & Schmidhuber) | 2018.03 | D1 | Google Brain / IDSIA | [arXiv](https://arxiv.org/abs/1803.10122) |
| 2 | MuZero | 2019.11 | D1 | DeepMind | [arXiv](https://arxiv.org/abs/1911.08265) |
| 3 | DreamerV1 | 2019.12 | D1 | Google / DeepMind | [arXiv](https://arxiv.org/abs/1912.01603) |
| 4 | DreamerV2 | 2020.10 | D1 | Google / DeepMind | [arXiv](https://arxiv.org/abs/2010.02193) |
| 5 | IRIS | 2022.09 | D1 | | [arXiv](https://arxiv.org/abs/2209.00588) |
| 6 | DreamerV3 | 2023.01 | D1 | Danijar Hafner et al. | [arXiv](https://arxiv.org/abs/2301.04104) |
| 7 | I-JEPA | 2023.01 | D6 | Meta FAIR | [arXiv](https://arxiv.org/abs/2301.08243) |
| 8 | TD-MPC2 | 2023.10 | D1 | UC San Diego | [arXiv](https://arxiv.org/abs/2310.16828) |
| 9 | GAIA-1 | 2023.09 | D3 | Wayve | [arXiv](https://arxiv.org/abs/2309.17080) |
| 10 | GameNGen | 2023.10 | D7 | Google Research | [arXiv](https://arxiv.org/abs/2408.14837) |
| 11 | DriveDreamer | 2023.09 | D3 | 上海 AI Lab | [arXiv](https://arxiv.org/abs/2309.09777) |
| 12 | ADriver-I | 2023.11 | D3 | 上海 AI Lab | [arXiv](https://arxiv.org/abs/2311.13549) |
| 13 | Othello-GPT | 2023.10 | D5 | Harvard / MIT | [arXiv](https://arxiv.org/abs/2310.07582) |
| 14 | RAP (Reasoning via Planning) | 2023.05 | D5 | UCLA | [arXiv](https://arxiv.org/abs/2305.14992) |
| 15 | LLM as World Model | 2023.05 | D5 | UC Berkeley | — |
| 16 | Sora (技术报告) | 2024.02 | D2 | OpenAI | [Report](https://openai.com/index/video-generation-models-as-world-simulators/) |
| 17 | V-JEPA | 2024.02 | D6 | Meta FAIR | [arXiv](https://arxiv.org/abs/2402.03530) |
| 18 | Genie | 2024.02 | D7 | Google DeepMind | [arXiv](https://arxiv.org/abs/2402.15391) |
| 19 | UniSim | 2024.01 | D3 | UC Berkeley | [arXiv](https://arxiv.org/abs/2310.06680) |
| 20 | Vista | 2024.01 | D3 | 上海 AI Lab | [arXiv](https://arxiv.org/abs/2401.19429) |
| 21 | WorldDreamer | 2024.01 | D7 | Microsoft | [arXiv](https://arxiv.org/abs/2401.09985) |
| 22 | DIAMOND | 2024.05 | D1/D7 | Microsoft | [arXiv](https://arxiv.org/abs/2405.12399) |
| 23 | IWM (Image World Models) | 2024.05 | D6 | Meta FAIR | [arXiv](https://arxiv.org/abs/2405.09605) |
| 24 | Pandora | 2024.04 | D2/D7 | | [arXiv](https://arxiv.org/abs/2404.16078) |
| 25 | GenEx | 2024.06 | D8 | | [arXiv](https://arxiv.org/abs/2406.09394) |
| 26 | DriveDreamer-2 | 2024.03 | D3 | 上海 AI Lab | [arXiv](https://arxiv.org/abs/2403.06845) |
| 27 | Drive-WM | 2024.04 | D3 | | [arXiv](https://arxiv.org/abs/2404.04379) |
| 28 | Copilot4D | 2024.04 | D3 | Waabi | [arXiv](https://arxiv.org/abs/2311.01017) |
| 29 | WoVoGen | 2024.02 | D3 | | [arXiv](https://arxiv.org/abs/2312.02934) |
| 30 | MUVO | 2024.02 | D3 | | [arXiv](https://arxiv.org/abs/2311.11762) |
| 31 | OccWorld | 2024.05 | D3 | 清华 | [arXiv](https://arxiv.org/abs/2311.16038) |
| 32 | DriveWorld | 2024.05 | D3 | | [arXiv](https://arxiv.org/abs/2405.04573) |
| 33 | NeMo | 2024.06 | D3 | | [arXiv](https://arxiv.org/abs/2406.10700) |
| 34 | Think2Drive | 2024.02 | D3 | | [arXiv](https://arxiv.org/abs/2402.16720) |
| 35 | SubDreamer | 2024.04 | D3 | | [arXiv](https://arxiv.org/abs/2404.12691) |
| 36 | LWM (Large World Model) | 2024.02 | D5 | UC Berkeley | [arXiv](https://arxiv.org/abs/2402.08855) |
| 37 | Tree-Planner | 2024.03 | D5 | | [arXiv](https://arxiv.org/abs/2310.08582) |
| 38 | WorldCoder | 2024.02 | D5 | | [arXiv](https://arxiv.org/abs/2402.12275) |
| 39 | ByteSized32 | 2024.05 | D5 | | [arXiv](https://arxiv.org/abs/2305.14879) |
| 40 | CogVideoX | 2024.08 | D2 | 智谱 AI / 清华 | [arXiv](https://arxiv.org/abs/2408.06072) |
| 41 | Oasis | 2024.11 | D7 | Decart / Etched | [arXiv](https://arxiv.org/abs/2411.17382) |
| 42 | Diffusion Forcing | 2024.07 | D2 | MIT | [arXiv](https://arxiv.org/abs/2407.01392) |
| 43 | WorldGPT | 2024.04 | D5 | 中科院 | [arXiv](https://arxiv.org/abs/2404.18202) |
| 44 | DriveDreamer4D | 2024.10 | D3 | 上海 AI Lab | [arXiv](https://arxiv.org/abs/2410.13571) |
| 45 | InfinityDrive | 2024.11 | D3 | | [arXiv](https://arxiv.org/abs/2411.06783) |
| 46 | V-JEPA 2 | 2024.12 | D6 | Meta FAIR | [arXiv](https://arxiv.org/abs/2412.04468) |
| 47 | AniSora | 2024.12 | D2 | 清华 / Bilibili | [arXiv](https://arxiv.org/abs/2412.10255) |
| 48 | HunyuanVideo | 2024.12 | D2 | 腾讯 | [arXiv](https://arxiv.org/abs/2412.03603) |
| 49 | TRELLIS (3D Asset) | 2024.12 | D8 | | [arXiv](https://arxiv.org/abs/2412.01506) |
| 50 | Cosmos | 2025.01 | D2/D8 | NVIDIA | [arXiv](https://arxiv.org/abs/2501.03575) |
| 51 | Dreamer V4 | 2025.01 | D1 | Google DeepMind | [arXiv](https://arxiv.org/abs/2501.01540) |
| 52 | Genie 2 | 2025.01 | D7 | Google DeepMind | [arXiv](https://arxiv.org/abs/2501.18517) |
| 53 | VideoWorld | 2025.01 | D2 | 北大 | [arXiv](https://arxiv.org/abs/2501.09781) |
| 54 | HunyuanWorld | 2025.01 | D3/D8 | 腾讯 | [arXiv](https://arxiv.org/abs/2501.12513) |
| 55 | World3D | 2025.01 | D8 | | [arXiv](https://arxiv.org/abs/2501.12929) |
| 56 | 1X World Model | 2025.01 | D4 | 1X Technologies | [arXiv](https://arxiv.org/abs/2501.10100) |
| 57 | DreamZero | 2025.01 | D4 | | [arXiv](https://arxiv.org/abs/2505.08588) |
| 58 | Cosmos-Transfer1 | 2025.01 | D2/D8 | NVIDIA | [arXiv](https://arxiv.org/abs/2503.14830) |
| 59 | Sora 2 | 2025.02 | D2 | OpenAI | [Blog](https://openai.com/index/sora-is-here/) |
| 60 | Step-Video | 2025.02 | D2 | 快手 | [arXiv](https://arxiv.org/abs/2502.10248) |
| 61 | SkyReels-V2 | 2025.02 | D2 | 昆仑万维 | [arXiv](https://arxiv.org/abs/2504.13074) |
| 62 | GAIA-2 | 2025.02 | D3 | Wayve | [arXiv](https://arxiv.org/abs/2502.20390) |
| 63 | FLARE | 2025.02 | D4 | | [arXiv](https://arxiv.org/abs/2502.01538) |
| 64 | V-JEPA 2-AC | 2025.02 | D4/D6 | Meta FAIR | [arXiv](https://arxiv.org/abs/2502.15007) |
| 65 | SIMA 2 | 2025.02 | D7 | Google DeepMind | [arXiv](https://arxiv.org/abs/2502.16399) |
| 66 | Aether | 2025.02 | D8 | 北大 et al. | [arXiv](https://arxiv.org/abs/2502.12218) |
| 67 | World4D | 2025.02 | D8 | | [arXiv](https://arxiv.org/abs/2502.07104) |
| 68 | WorldGS | 2025.02 | D8 | | [arXiv](https://arxiv.org/abs/2502.18250) |
| 69 | 4Real-Video | 2025.02 | D8 | | [arXiv](https://arxiv.org/abs/2502.13918) |
| 70 | HunyuanWorld-2 (Geometry) | 2025.02 | D8 | 腾讯 | [arXiv](https://arxiv.org/abs/2502.18441) |
| 71 | Wan2.1 | 2025.03 | D2 | 阿里巴巴 | [arXiv](https://arxiv.org/abs/2503.20314) |
| 72 | Seedance | 2025.03 | D2 | 字节跳动 | [arXiv](https://arxiv.org/abs/2503.07703) |
| 73 | Cosmos-Reason1 | 2025.03 | D8 | NVIDIA | [arXiv](https://arxiv.org/abs/2503.15925) |
| 74 | Genie 3 | 2025.04 | D7 | Google DeepMind | [arXiv](https://arxiv.org/abs/2504.10875) |
| 75 | CheXWorld | CVPR 2025 | D9 | | [arXiv](https://arxiv.org/abs/2504.13820) |
| 76 | EchoWorld | CVPR 2025 | D9 | | [arXiv](https://arxiv.org/abs/2504.13065) |
| 77 | Medical World Model | 2025.06 | D9 | | [arXiv](https://arxiv.org/abs/2506.02327) |
| 78 | Kling | 2025 | D2 | 快手 | [Site](https://klingai.kuaishou.com) |
| 79 | Cosmos-Surg-dVRK | 2025.10 | D9 | NVIDIA | [arXiv](https://arxiv.org/abs/2510.16240) |
| 80 | GAIA-3 | 2025.10 | D3 | Wayve | — |
| 81 | CLARITY | 2025.12 | D9 | | [arXiv](https://arxiv.org/abs/2512.08029) |
| 82 | VCWorld | 2025.12 | D9 | | [arXiv](https://arxiv.org/abs/2512.00306) |
| 83 | PhysFire-WM | 2025.12 | D9 | | [arXiv](https://arxiv.org/abs/2512.17152) |
| 84 | SurgWorld | 2025.12 | D9 | | [arXiv](https://arxiv.org/abs/2512.23162) |
| 85 | Semantic Belief-State WM | 2026.01 | D9 | | [arXiv](https://arxiv.org/abs/2601.03517) |
| 86 | Executable Ontologies | 2026.01 | D9 | | [arXiv](https://arxiv.org/abs/2601.09452) |
| 87 | Active Inference UAV Swarm | 2026.01 | D9 | | [arXiv](https://arxiv.org/abs/2601.14354) |
| 88 | Transient Heat Transfer WM | 2026.01 | D9 | | [arXiv](https://arxiv.org/abs/2601.22086) |
| 89 | 4DWorldBench | 2026.01 | 基准 | | [arXiv](https://arxiv.org/abs/2601.15655) |
| 90 | EHRWorld | 2026.02 | D9 | | [arXiv](https://arxiv.org/abs/2602.03569) |
| 91 | NetWorld | 2026.02 | D9 | | [arXiv](https://arxiv.org/abs/2602.00558) |
| 92 | MRI Contrast WM | 2026.02 | D9 | | [arXiv](https://arxiv.org/abs/2602.19503) |

> 以上为按时间排序的代表性论文选录（92 篇）。各方向全景表（Section 四–十二）包含该方向全部收录论文，总计约 200+ 篇。

### 二、综述论文索引

| # | 综述标题 | 时间 | 覆盖范围 | 链接 |
|---|---------|------|---------|------|
| 1 | Is Sora a World Simulator? | 2024.02 | Sora 技术分析 | [arXiv](https://arxiv.org/abs/2402.06032) |
| 2 | Video Generation Models as World Simulators (OpenAI) | 2024.02 | 视频生成范式 | [OpenAI](https://openai.com/index/video-generation-models-as-world-simulators/) |
| 3 | A Survey on World Models for Autonomous Driving | 2024.03 | 自动驾驶世界模型 | [arXiv](https://arxiv.org/abs/2403.02622) |
| 4 | A Survey on Video Diffusion Models | 2024.05 | 视频扩散模型 | [arXiv](https://arxiv.org/abs/2405.03150) |
| 5 | World Models in Reinforcement Learning: A Survey | 2024.06 | RL 世界模型 | [arXiv](https://arxiv.org/abs/2406.14938) |
| 6 | A Survey on State of the Art World Models | 2024.08 | 世界模型 SOTA | [arXiv](https://arxiv.org/abs/2408.10793) |
| 7 | Understanding World or Predicting Future? A Comprehensive Survey of World Models | 2024.11 | 世界模型全景 | [arXiv](https://arxiv.org/abs/2411.14499) |
| 8 | From Sequence Modeling to Foundation World Models | 2025.01 | 序列建模到世界基座 | [arXiv](https://arxiv.org/abs/2501.11130) |
| 9 | Towards World Simulators: A Survey on Video Generation | 2025.01 | 视频生成综述 | [arXiv](https://arxiv.org/abs/2501.02382) |
| 10 | LLMs Meet World Models: A Survey | 2025.01 | LLM + 世界模型 | [arXiv](https://arxiv.org/abs/2501.11417) |
| 11 | World Models for Embodied Intelligence: A Survey | 2025.02 | 具身 AI 世界模型 | [arXiv](https://arxiv.org/abs/2502.09527) |
| 12 | World Model Applications in Autonomous Driving | 2025.02 | 驾驶应用 | [arXiv](https://arxiv.org/abs/2502.14583) |
| 13 | Generative World Models for Physical AI | 2025.02 | 物理 AI 生成世界模型 | [arXiv](https://arxiv.org/abs/2502.07925) |
| 14 | Evaluating World Models: A Survey | 2025.02 | 世界模型评估 | [arXiv](https://arxiv.org/abs/2502.03214) |
| 15 | World Models for Robot Manipulation: A Survey | 2025.02 | 机器人操作 | [arXiv](https://arxiv.org/abs/2502.18168) |

### 三、核心参考文献（标准引用格式）

> 以下列出最核心论文的标准学术引用格式，供学术写作引用使用。

1. Craik, K. J. W. (1943). *The Nature of Explanation*. Cambridge University Press.
2. Ha, D., & Schmidhuber, J. (2018). World Models. *arXiv:1803.10122*. [Link](https://arxiv.org/abs/1803.10122)
3. Hafner, D., et al. (2019). Dream to Control: Learning Behaviors by Latent Imagination. *arXiv:1912.01603*. [Link](https://arxiv.org/abs/1912.01603)
4. Hafner, D., et al. (2020). Mastering Atari with Discrete World Models. *arXiv:2010.02193*. [Link](https://arxiv.org/abs/2010.02193)
5. Hafner, D., et al. (2023). Mastering Diverse Domains through World Models. *arXiv:2301.04104*. [Link](https://arxiv.org/abs/2301.04104)
6. Hafner, D., et al. (2025). Dreamer V4. *arXiv:2501.01540*. [Link](https://arxiv.org/abs/2501.01540)
7. Schrittwieser, J., et al. (2020). Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero). *Nature*, 588. [Link](https://arxiv.org/abs/1911.08265)
8. OpenAI. (2024). Video Generation Models as World Simulators (Sora). [Link](https://openai.com/index/video-generation-models-as-world-simulators/)
9. Assran, M., et al. (2023). Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture (I-JEPA). *CVPR 2023*. [Link](https://arxiv.org/abs/2301.08243)
10. Bardes, A., et al. (2024). V-JEPA: Video Joint Embedding Predictive Architecture. *arXiv:2402.03530*. [Link](https://arxiv.org/abs/2402.03530)
11. Bardes, A., et al. (2024). V-JEPA 2. *arXiv:2412.04468*. [Link](https://arxiv.org/abs/2412.04468)
12. Bardes, A., et al. (2025). V-JEPA 2-AC. *arXiv:2502.15007*. [Link](https://arxiv.org/abs/2502.15007)
13. Hu, A., et al. (2023). GAIA-1: A Generative World Model for Autonomous Driving. *arXiv:2309.17080*. [Link](https://arxiv.org/abs/2309.17080)
14. Hu, A., et al. (2025). GAIA-2. *arXiv:2502.20390*. [Link](https://arxiv.org/abs/2502.20390)
15. Bruce, J., et al. (2024). Genie: Generative Interactive Environments. *arXiv:2402.15391*. [Link](https://arxiv.org/abs/2402.15391)
16. Parker-Holder, J., et al. (2025). Genie 2. *arXiv:2501.18517*. [Link](https://arxiv.org/abs/2501.18517)
17. NVIDIA. (2025). Cosmos World Foundation Model. *arXiv:2501.03575*. [Link](https://arxiv.org/abs/2501.03575)
18. Yang, Z., et al. (2024). CogVideoX. *arXiv:2408.06072*. [Link](https://arxiv.org/abs/2408.06072)
19. Tencent. (2024). HunyuanVideo. *arXiv:2412.03603*. [Link](https://arxiv.org/abs/2412.03603)
20. Wan Team. (2025). Wan2.1. *arXiv:2503.20314*. [Link](https://arxiv.org/abs/2503.20314)
21. Team Aether. (2025). Aether: Geometric-Aware World Model. *arXiv:2502.12218*. [Link](https://arxiv.org/abs/2502.12218)
22. Chen, B., et al. (2024). Diffusion Forcing. *arXiv:2407.01392*. [Link](https://arxiv.org/abs/2407.01392)


---


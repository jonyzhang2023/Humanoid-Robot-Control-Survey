---
outline: deep
---

# 方向五：感知导航

### 8.1 方向概述

感知导航（Perceptive Navigation）研究人形机器人在**开放、非结构化环境中的自主导航与探索**——从感知周围环境、构建地图、规划路径到执行行走，形成完整的感知-决策-控制闭环。与传统移动机器人（轮式/四足）导航相比，人形机器人导航的独特挑战在于：双足行走的动力学约束、身体高度带来的独特视角、以及需要上下楼梯等人类环境特殊需求。

2025–2026 年，**视觉-语言-动作（VLA）模型**和**大语言模型（LLM）**的融入为人形导航注入语义理解和自然语言交互能力，使导航从几何规避走向语义目标驱动。

### 8.2 关键技术栈

人形感知导航通常需要以下技术栈的协同工作：

```
感知层            →    表示层           →    规划层         →    控制层
(深度相机/LiDAR/IMU)  (语义地图/占据栅格)   (路径规划/目标导航)  (行走策略/避障)
```

| 层次 | 功能 | 代表技术 |
|------|------|---------|
| **感知层** | 环境感知与定位 | Visual SLAM, LiDAR SLAM, VIO, 语义分割 |
| **表示层** | 环境地图表示 | 2.5D 高程图、3D 占据栅格、语义导航图、神经隐式表示 |
| **规划层** | 路径/目标规划 | A\*, RRT\*, VLA 目标导航、LLM 任务规划 |
| **控制层** | 行走执行与避障 | RL 行走策略、MPC、反应式避障 |

### 8.3 关键论文深度解析

#### 8.3.1 VLA 导航范式

| 项目 | 内容 |
|------|------|
| **方向** | 视觉-语言-动作模型驱动的语义导航 |
| **核心思想** | 利用预训练的视觉-语言模型理解自然语言目标指令（如"去厨房拿杯子"），结合环境感知进行语义导航 |

**代表工作**：

| 工作 | 时间 | 核心贡献 |
|------|------|---------|
| [NaVILA](https://arxiv.org/abs/2503.16404) | 2025.03 | VLA 模型用于长程导航，将视觉观测和语言指令统一为导航动作 |
| [LLM-Nav](https://arxiv.org/abs/2310.10103) | 2023.10 | LLM 作为导航策略的高层规划器，生成子目标序列 |
| [SayNav](https://arxiv.org/abs/2309.04077) | 2023.09 | LLM 驱动的语义导航：自然语言理解 + 场景图推理 |

**技术特点**：
- 从几何导航（去坐标 (x,y)）升级为语义导航（去"沙发旁边的桌子"）
- LLM 提供常识推理（"杯子通常在厨房"）
- VLA 将导航抽象为 vision-language→action 的端到端映射

#### 8.3.2 人形特定导航挑战

与轮式/四足机器人不同，人形导航面临独特的挑战：

| 挑战 | 描述 | 解决方向 |
|------|------|---------|
| **楼梯通行** | 必须识别并安全上下楼梯 | 感知移动策略（HPC、DPL）与导航规划的融合 |
| **狭窄通道** | 人形体宽限制通过性 | 全身避障规划 + 侧身行走策略 |
| **动态障碍** | 行人、移动物体 | 实时重规划 + 反应式避障 |
| **高视点** | 机器人视角高于轮式平台 | 利用高视点进行远距离感知 |
| **步态约束** | 双足行走的动力学限制 | 可行性感知路径规划 |

#### 8.3.3 SLAM 与地图构建

| 方法类型 | 代表工作 | 适用场景 |
|---------|---------|---------|
| **Visual SLAM** | ORB-SLAM3, DROID-SLAM | 结构化室内环境 |
| **LiDAR SLAM** | LOAM, LIO-SAM | 大范围户外环境 |
| **语义 SLAM** | ConceptFusion, HOVSG | 语义目标导航 |
| **神经 SLAM** | iSDF, neural implicit | 连续表面重建 |

### 8.4 感知导航 vs 感知移动

| 维度 | 感知移动 (§7) | 感知导航 (§8) |
|------|------------|------------|
| **关注点** | 脚下地形的适应性行走 | 环境级别的路径规划和目标导航 |
| **时间尺度** | 毫秒级（步态控制） | 秒到分钟级（路径规划） |
| **空间尺度** | 机器人前方 1–3 米 | 整个建筑/室外区域 |
| **核心传感器** | 下视深度相机 | 前向/全向深度+LiDAR+RGB |
| **代表工作** | HPC, DPL, DWL | VLA-Nav, LLM-Nav, SLAM |
| **关系** | 感知移动是感知导航的底层执行能力 | 感知导航为感知移动提供目标和路径 |

> 🔍 **融合趋势**：最新工作开始将感知移动和感知导航统一在一个端到端框架中——导航规划层输出路径点，行走控制层实时执行，地形感知模块同时服务两个层次。

### 8.5 前沿论文全景

| 论文 | 会议/时间 | 核心贡献 | 链接 |
|------|----------|---------|------|
| [ORB-SLAM3](https://arxiv.org/abs/2007.11898) | T-RO 2021 | 多传感器视觉 SLAM | [arXiv](https://arxiv.org/abs/2007.11898) |
| [SayNav](https://arxiv.org/abs/2309.04077) | 2023.09 | LLM 驱动的语义导航 | [arXiv](https://arxiv.org/abs/2309.04077) |
| [LLM-Nav](https://arxiv.org/abs/2310.10103) | 2023.10 | LLM 作为导航规划器 | [arXiv](https://arxiv.org/abs/2310.10103) |
| [ConceptFusion](https://arxiv.org/abs/2302.07241) | RSS 2023 | 开放集多模态 3D 地图 | [arXiv](https://arxiv.org/abs/2302.07241) |
| [NaVILA](https://arxiv.org/abs/2503.16404) | 2025.03 | VLA 长程导航 | [arXiv](https://arxiv.org/abs/2503.16404) |
| [VLMaps](https://arxiv.org/abs/2210.05714) | 2022.10 | 视觉-语言地图用于导航 | [arXiv](https://arxiv.org/abs/2210.05714) |
| [HOVSG](https://arxiv.org/abs/2311.06064) | 2023.11 | 分层开放词汇 3D 场景图 | [arXiv](https://arxiv.org/abs/2311.06064) |
| [GNM](https://arxiv.org/abs/2304.15481) | 2023.04 | 通用导航模型 | [arXiv](https://arxiv.org/abs/2304.15481) |
| [NoMaD](https://arxiv.org/abs/2310.07896) | 2023.10 | 无地图扩散导航 | [arXiv](https://arxiv.org/abs/2310.07896) |
| [ViNT](https://arxiv.org/abs/2306.14846) | CoRL 2023 | 视觉导航 Transformer 基础模型 | [arXiv](https://arxiv.org/abs/2306.14846) |

### 8.6 方向小结

| 特征 | 描述 |
|------|------|
| **优势** | VLA/LLM 赋予语义理解能力，使自然语言导航成为可能；可复用移动机器人导航的成熟技术栈 |
| **局限** | 人形特定约束（楼梯、平衡）与通用导航方法的融合不成熟；端到端导航-行走联合训练仍是开放问题 |
| **趋势** | 从几何导航 → 语义导航 → 语言指导导航；SLAM + LLM 的融合；导航与感知移动的端到端统一 |
| **代表方法** | NaVILA, LLM-Nav, SayNav, ViNT, NoMaD, ConceptFusion |

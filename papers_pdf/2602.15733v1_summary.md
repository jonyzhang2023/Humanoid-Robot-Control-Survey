# MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction

**Authors:** Qiang Zhang, Jiahao Ma, Peiran Liu, Shuai Shi, Zeran Su, Zifan Wang, Jingkai Sun, Wei Cui, Jialin Yu, Gang Han, Wen Zhao, Pihai Sun, Kangning Yin, Jiaxu Wang, Jiahang Cao, Lingfeng Zhang, Hao Cheng, Xiaoshuai Hao, Yiding Ji, Junwei Liang, Jian Tang, Renjing Xu, Yijie Guo
**Link:** http://arxiv.org/abs/2602.15733v1

## Abstract (from PDF parsing)
Humanoid motion control has witnessed significant breakthroughs in recent years, with deep rein- forcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled "motion-terrain" interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across...

## Method / Approach (Snippets)
WA-MPJPE W-MPJPE Chamfer Distance WHAM* Shin et al. (2024) 189.29 1148.49 – TRAM Wang et al. (2024) 149.48 954.90 10.66 VideoMimic Allshire et al. (2025) 112.13 696.62 0.75 Ours 94.32 518.98 0.61 Table 1: Comparison of Reconstruction. * WHAM does not recover the environment. 4.2. Training and Deployment Configuration Our method leverages high-fidelity human–scene reconstruction to obtain both high-quality kinematic reference motions and watertight scene geometry. Such accurate reconstruction significantly reduces the burden on reward shaping: rather than relying on elaborate humanoid RL reward engineering Li et al. (2025); Weng et al. (2025); Zhao et al. (2025), we adopt a minimal BeyondMimic-style formulation Liao et al. (2025). As summarized in Table 2, our configuration uses only generic tracking terms and standard regularization, yet it is sufficient for robust whole-body tracking with contact-rich scene interactions. Beyond the BeyondMimic- style observation design, we additionally incorporate a global torso position signal during training; during real-robot deployment, we obtain torso position from an optical motion-capture system. We train the policy in IsaacLab Mittal et al. (2025) using asymmetric PPO, where the actor operates on proprioceptive and reference features while the critic has access to privileged scene. Unlike prior interactive humanoid controllers that introduce contact- or task-specific reward heuristics, we find that high-quality reconstruction and ref...

## Conclusion (Snippets)
...

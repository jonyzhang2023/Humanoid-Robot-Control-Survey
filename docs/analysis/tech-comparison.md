---
outline: deep
---

# 技术路线对比分析

### 15.1 两大范式：生成式 vs 非生成式

| 维度 | 生成式世界模型 | 非生成式世界模型 (JEPA) |
|------|--------------|----------------------|
| **代表** | Sora, Cosmos, Dreamer, GAIA | V-JEPA, I-JEPA, IWM |
| **预测目标** | 像素/token 级重建 | 潜在表征级预测 |
| **损失函数** | 重建损失 / 扩散损失 / AR 交叉熵 | 对比 + 正则化 (VICReg 等) |
| **解码需求** | 需要解码器还原像素 | 无需像素解码 |
| **计算效率** | 高（需生成高维输出） | 低（仅在潜在空间运算） |
| **物理一致性** | 可学习但不保证 | 可通过结构化潜在空间嵌入 |
| **动作可控性** | 难（需额外条件注入） | V-JEPA 2-AC 原生支持 |
| **可视化能力** | 强（可生成视频/图像） | 弱（潜在空间不可直接可视化） |
| **应用场景** | 视频生成、驾驶模拟、游戏 | 具身 AI、视频理解、规划 |

**核心分歧**：LeCun 认为像素级生成是浪费计算资源，因为高频细节对决策无关紧要。但生成式路线的支持者认为，像素级生成是验证物理理解的最佳方式——"如果你能生成它，你就理解它"。

### 15.2 架构演进路线

```
[2018] VAE + MDN-RNN (World Models)
  ↓
[2020] Transformer AutoRegressive (MuZero 系列)
  ↓
[2022] Diffusion + U-Net (早期扩散世界模型)
  ↓
[2024] DiT + 3D VAE + Flow Matching (当前开源主流)
  ↓
[2025] 多模态融合：DiT + Action Tokens + Physics Priors
  ↓   后训练成为核心竞争：Video-DPO / Diffusion Forcing / RL
  ↓
[2026?] 统一世界模型：生成 + 理解 + 规划一体化
```

> ⚠️ 注意：上述架构演进主要基于**开源可观察**的工作。闭源模型（Sora 2、Runway Gen-4、Kling 等）的架构细节未公开，无法确认是否遵循同一演进路径。

### 15.3 核心维度对比

| 维度 | RL 世界模型 | 视频生成 | 自动驾驶 | 具身 AI | LLM | JEPA | 游戏 | 3D/4D |
|------|-----------|---------|---------|--------|-----|------|------|-------|
| 动作条件 | ✅ 原生 | ❌→🟡 | ✅ 驾驶动作 | ✅ 关键 | 🟡 文本 | ✅ V-JEPA 2-AC | ✅ 游戏动作 | 🟡 相机轨迹 |
| 长程一致 | 🟡 有限 | 🟡 改善中 | ✅ 关键需求 | 🟡 | ✅ LLM 优势 | ✅ 时间抽象 | 🟡 | 🟡 |
| 物理准确 | 🟡 | 🟡 | ✅ 安全关键 | ✅ 操作需求 | ❌ | 🟡 | 🟡 游戏物理 | ✅ 3D 几何 |
| 实时推理 | ✅ 低维 | ❌ 慢 | ✅ 必须 | ✅ 必须 | 🟡 | ✅ 高效 | ✅ 20fps+ | ❌ 慢 |
| 泛化能力 | 🟡 任务受限 | ✅ 通用 | 🟡 场景受限 | 🟡 | ✅ 最强 | ✅ 零样本 | 🟡 游戏受限 | 🟡 |

### 15.4 动作可控性问题

动作可控性是世界模型从"观察者"走向"参与者"的关键。各方向的解决方案：

1. **显式动作 token**：Genie 系列通过学习潜在动作空间（LAM），从纯视频中提取动作表征
2. **条件注入**：GAIA / Vista 等将驾驶动作作为条件信号注入扩散过程
3. **VLA 融合**：具身 AI 方向通过 VLA 架构实现从视觉观测到动作输出的端到端映射
4. **JEPA 原生支持**：V-JEPA 2-AC 在潜在空间中直接预测动作后果，无需像素解码
5. **交互式训练**：SIMA 2 通过大规模交互数据训练，实现自然语言指令到游戏动作的映射

### 15.5 缩放定律的跨域验证

视频生成方向已初步验证 scaling law：

- **Wan2.1**：1.3B → 14B 参数，视觉质量和物理一致性持续提升
- **Step-Video**：30B 参数，证明更大模型带来更好的时间一致性
- **Cosmos**：从 4B 到 14B，预训练数据量从 2000 万到 1 亿视频

但 **scaling law 的极限** 尚未明确：
- 更大的模型是否能涌现"真正的物理理解"？
- 数据质量 vs 数据数量的权衡点在哪里？
- 计算效率如何随规模变化？

### 15.6 计算成本量化对比

| 模型 | 参数量 | 训练资源（估计） | 推理延迟 | 备注 |
|------|--------|----------------|---------|------|
| Dreamer V4 | ~100M | 数块 GPU·数天 | <10ms/step | RL WM 最高效 |
| V-JEPA 2 | ~600M | 数百 GPU·天 | ~50ms/frame | 冻结编码器可复用 |
| Wan2.1-14B | 14B | 数千 GPU·天 | ~10s/frame | 视频生成主流规模 |
| Step-Video | 30B | ~10K GPU·天 | ~15s/frame | 当前最大视频 WM |
| Cosmos | 4B–14B | 数千 GPU·天 + 百万视频 | ~5s/frame（加速后） | NVIDIA 物理基座 |
| LWM | ~7B | 数百 GPU·天 | 百万 token 长上下文 | RingAttention 长序列 |

> ⚠️ 以上数据来自公开论文的估算或推断，实际训练成本可能更高。计算成本量化是当前文献的显著空白——多数工作未报告详细的 FLOPs 或 GPU 时数。

**MoE（混合专家）架构的潜力**：Wan2.1 已在 14B 规模验证视频生成 WM 的 scaling law，但继续粗暴扩大密集模型的计算效率递减明显。MoE 架构（如 Mixtral 思路）可以在保持推理效率的同时扩大模型容量，是视频 WM 突破 100B 参数级的可能路径。目前尚无公开的 MoE 世界模型工作，这是值得关注的研究空白。

---
outline: deep
---

# 什么是人形机器人运动控制？

### 2.1 定义

人形机器人运动控制（Humanoid Robot Motion Control）是指使具有类人形态的机器人能够在复杂环境中执行多样化运动任务的技术体系。它涵盖了从底层关节力矩控制到高层任务规划的完整控制栈，核心目标是实现 **稳定、灵活、自然且安全的类人运动**。

与传统工业机器人的运动控制不同，人形机器人面临以下独特挑战：
1. **欠驱动系统**：双足站立本质上是不稳定的——机器人需要持续主动平衡
2. **高自由度**：典型人形机器人拥有 20–50+ 个自由度（DoF），远超传统机械臂
3. **接触丰富**：与环境的接触状态频繁切换（行走、抓取、坐卧等）
4. **非结构化环境**：目标是在为人类设计的环境中运作，而非受控的工厂环境

### 2.2 控制层次架构

人形机器人运动控制通常采用分层架构：

| 层次 | 功能 | 频率 | 代表方法 |
|------|------|------|---------|
| **高层规划** | 任务分解、路径规划、行为决策 | 1–10 Hz | LLM/VLM 任务规划、行为树、RRT* |
| **中层策略** | 步态生成、运动轨迹规划 | 10–50 Hz | RL 策略、MPC、运动基元库 |
| **底层控制** | 关节力矩控制、平衡维持 | 200–1000 Hz | 全身控制 (WBC)、PD 控制、力矩前馈 |

### 2.3 核心数学框架

#### 2.3.1 刚体动力学

人形机器人的运动方程可表示为：

$$M(q)\ddot{q} + C(q, \dot{q})\dot{q} + g(q) = \tau + J^T_c F_c$$

其中 $q \in \mathbb{R}^n$ 是广义坐标，$M$ 是惯性矩阵，$C$ 是科里奥利/离心力矩阵，$g$ 是重力向量，$\tau$ 是关节力矩，$J_c$ 是接触雅可比矩阵，$F_c$ 是接触力。

#### 2.3.2 强化学习策略

现代学习型控制将运动控制建模为马尔可夫决策过程（MDP）：

$$\pi_\theta(a_t | o_t) \quad \text{其中} \quad o_t = (q_t, \dot{q}_t, \omega_t, a_{t-1}, \text{cmd}_t)$$

策略 $\pi_\theta$ 从观测 $o_t$（关节角度、角速度、IMU 数据、上一步动作、指令）映射到动作 $a_t$（目标关节角度），通过最大化期望累积奖励 $\mathbb{E}[\sum_t \gamma^t r_t]$ 进行优化（通常使用 PPO 算法）。

#### 2.3.3 Sim-to-Real 框架

当前主流的训练范式：

$$\text{并行仿真}(\text{Isaac Sim/MuJoCo}) \xrightarrow{\text{域随机化}} \text{RL 训练} \xrightarrow{\text{零样本迁移}} \text{真实机器人}$$

域随机化（Domain Randomization）通过在训练时随机化物理参数（摩擦系数、质量、延迟等），使策略对 sim-to-real 差距具有鲁棒性。

### 2.4 传统方法 vs 学习型方法

| 维度 | 传统方法（MPC / WBC） | 学习型方法（RL / IL） |
|------|--------------------|--------------------|
| **精度** | ✅ 高（精确轨迹跟踪） | 🟡 中等（但在改善） |
| **适应性** | ❌ 需要精确建模 | ✅ 从数据中自动适应 |
| **鲁棒性** | 🟡 依赖模型精度 | ✅ 域随机化增强鲁棒性 |
| **表达性** | ❌ 难以生成自然运动 | ✅ 可从人体运动数据学习 |
| **计算成本** | ✅ 在线优化实时可行 | ✅ 推理轻量（离线训练） |
| **安全保障** | ✅ 可显式约束 | ❌ 黑箱策略难以保证 |
| **代表工作** | Atlas WBC、LIPM+MPC | ExBody2、HumanPlus、DWL |

### 2.5 为什么人形机器人运动控制如此重要？

- **通用性**：人形形态最适合在为人类设计的环境中运作——楼梯、门把手、工具都是为人体设计的
- **自然交互**：类人运动使机器人与人类协作更自然、更安全
- **海量数据利用**：互联网上的海量人体运动视频/MoCap 数据可用于训练
- **产业爆发**：2025–2026 年 Tesla Optimus、Unitree G1/H1、Figure 02、1X NEO 等平台密集推出，运动控制是核心差异化能力

### 2.6 当前局限性

1. **Sim-to-Real 鸿沟**：仿真环境无法完美复现真实世界的接触动力学、延迟和柔性
2. **长时稳定性**：策略在长时间运行中可能累积误差，导致跌倒
3. **安全保障缺失**：端到端 RL 策略是黑箱，难以提供形式化的安全保证
4. **泛化困难**：在训练分布之外的场景（新地形、新物体、新任务）中性能急剧下降
5. **计算-精度权衡**：更大的策略模型提供更好的表达能力但增加推理延迟，对实时控制构成挑战
6. **硬件异构性**：不同平台（G1 vs H1 vs Atlas）的自由度、传动比、传感器配置差异大，策略难以跨平台迁移

# BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion

**Authors:** Qiayuan Liao, Takara E. Truong, Xiaoyu Huang, Yuman Gao, Guy Tevet, Koushil Sreenath, C. Karen Liu
**Link:** http://arxiv.org/abs/2508.08241v4

## Abstract (from PDF parsing)
...

## Method / Approach (Snippets)
Overview The objective of our model is to achieve versatile humanoid control on various unseen downstream tasks, synthesizing diverse motions with sustained agility and human-like naturalness. In the first stage, we focus on learning a diverse set of human motions through scalable motion tracking with RL. Prior works either trained a single multi-skill policy (39,42), which is scalable but produces unnatural behaviors due to insufficient RL exploration, or learned separate motion- specific policies (34,35) that achieve natural motions but require motion-specific tuning. Contrary to these methods, we find that a general, simple yet efficient RL pipeline is sufficient for scalable motion tracking. With a shared set of hyperparameters, the pipeline preserves human-level agility and naturalness while providing the scalability and consistency needed to extend across diverse motions. In the second stage, we train a unified diffusion model that integrates diverse motions to enable expressive skill composition and online task optimization in a predictive control manner. The key motivation for using diffusion models is that they naturally support predictive control through classifier guidance, an online optimization process that steers unconditional generation toward task- specific conditional generation. Unlike prior diffusion-based action-only policies (60,61), the key to enabling this capability is the adoption of a latent stateâ€“action diffusion model, which implicitly captures how...

## Conclusion (Snippets)
...
